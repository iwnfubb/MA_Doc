\chapter{Eigenes Verfahren}\label{chp:EigVerfahren}
Diese Arbeit besteht aus drei große Hauptschritte, nämlich Hintergrundsubtraktion, Schätzung der Körperhalterung mit Histogrammanalyse und Erkennung außergewöhnlicher Situation mit Fuzzylogik. Um mein eigenes Verfahren einfacher zu verstehen, stellt Die Abbildung \ref{fig:allgemein} allgemeinen Prozess  der drei Hauptschritte dar.

\begin{figure}[htpb]			
	\centering
	\def\svgwidth{0.5\textwidth}
	\import{./fig/}{allgemein.pdf_tex}
	\caption{Drei Hauptschritte zur Erkennung von außergewöhnlichen Situation}
	\label{fig:allgemein}
\end{figure}

\section{Hintergrundsubtraktion}\label{chp:BackgroundSubtraction}
\subsection{Vergleichen verschiedener Methoden}
In dieser Masterarbeit ist Hintergrundsubtraktion ein wichtiger Baustein. Ein großes Problem ist wie ein korrekter Hintergrund erstellt werden kann, damit man sich bewegtes Objekt robust erkennen kann. Für diese Arbeit wurden verschiedenen moderne Methoden (Gaussian Mixture Model, Kern Density Estimation und Vibe) ausprobiert und verglichen. Die alle genannte Methoden wurden schon in ~\ref{sec:grunglagen_hintergrundsub} theoretisch beschrieben. In diesem Abschnitt geht es um Bewertungen von Hintergrundsubtraktionsverfahren. Bei \acs{GMM} wird eine Mischung von 5-Gaußschen Verteilung modelliert und eine Anzahl von 100 letzte Frames wird als "History"\ angewendet. Mit dem Lernrate $0.01$ funktioniert das \acs{GMM} Verfahren gut in unserem Test. Für das \acs{AGMM} nehmen wir auch die gleichen Parameters, um den Vergleich der Hintergrundsubtraktionsverfahren objektiv zu bewerten. Bei \acs{KDE} handelt es sich um eine Berechnung von Intensitätswerte für einen Pixel, deshalb wird nur ein Parameter von 100 als "History" wie bei \acs{AGMM} in diesem Fall eingesetzt. Wie in \cite{barnich2009vibe} schon gemeint, Vibe ist ein nicht-parametrisches Verfahren und weshalb kein Parameter wird hier gebraucht. 

\begin{figure}[htpb]
	\centering
	\includegraphics[width=1\textwidth]{fig/motiondetrection.png}
	\caption{Vergleichen von verschiedene Techniken an Hintergrundsubtraktion (a) Weichzeichnen (b) GMM (c) AGMM. (d) KDE (e) Vibe} 
	\label{fig:compare_bgsubtraction}
\end{figure}
Aus dieser Grafik \ref{fig:compare_bgsubtraction} wird deutlich, dass AGMM und Vibe ziemlich besser als die zweit andere. Hier ist kritisch anzumerken, dass Vibe und KDE längere Verarbeitungszeit als den Rest brauchen. Das erste Ziel der Arbeit ist die Untersuchung der Hintergrundsubtraktion, damit eine beste Silhouette erkannt werden kann. Anhand des Schaubildes kann man erkennen, dass AGMM ein guter Kandidat für Hintergrundsubtraktion ist. Viele Videos in verschieden Orte mit verschiedene Körperhaltung wurden für diese Arbeit aufgenommen, damit die Qualität der Software gut geprüft wird.  Anhand der Abbildung \ref{fig:compare_bgsubtraction} und Verarbeitungszeit wird deutlich, dass AGMM vernünftige Methode für unser Ziel und deswegen wird AGMM für diese Arbeit entschieden.\\
Die oben genannte Hintergrundsubtraktionsverfahren basieren auf die Änderung des Intensitätswerten von jedem Pixel, deshalb liefern die Verfahren ein Schwarzweißbild mit viel Rausch zurück. Um die Rausch zu entfernen, wird Erosion und Dilatation zum Einsatz gebracht, die zwei Verfahren sind zwei grundlegende Operationen bei der morphologischen Bildverarbeitung, auf der alle anderen morphologischen Operationen basieren. Unter Erosion kann man verstehen ein Verfahren, das ein Bild mit einer einfachen, vordefinierten Form untersucht und daraus Schlussfolgerungen zieht, wie diese Form in die Formen im Bild verfehlt. Und Dilatation ist ein umgekehrtes Verfahren von Erosion, es versucht ein Bild zu erweitern mit passender Form von originalem Bild. Eingaben für die zwei Verfahren sind ein originales Bild $A$ und einen Filter $B$. Wenn das strukturierende Element $B$ ein Zentrum hat, kann die Erosion von A durch B als der Ort von Punkten verstanden werden, die durch das Zentrum von B erreicht werden, wenn B sich in A bewegt. Angenommen, dass der Ursprung $B$ in seiner Mitte ist, überlagert für jedes Pixel in $A$ der Ursprung von $B$, wenn $B$ vollständig in $A$ enthalten ist, wird das Pixel bei Erosion beibehalten, ansonsten gelöscht. Aber bei Dilatation überlagert jeder Pixel in $A$ der Ursprung von $B$ und der Pixel wird in der Erweiterung von $A$ und $B$ enthalten. Um die zwei Operationen besser zu verstehen, wurden zwei Beispiele für Erosion und Dilatation in Abbildungen \ref{fig:erosion} und \ref{fig:dilatation} dargestellt.
\begin{figure}[htpb]
	\centering
	\includegraphics[width=1\textwidth]{fig/erosion.jpg}
	\caption{Erosion} 
	\label{fig:erosion}
\end{figure}
\begin{figure}[htpb]
	\centering
	\includegraphics[width=1\textwidth]{fig/dilatation.jpg}
	\caption{Dilatation} 
	\label{fig:dilatation}
\end{figure}

Erosion wird in diesem Fall angewendet, um Rausch nach dem Hintergrundsubtraktion zu entfernen. Es folgt auch eine kleine Änderung an der Silhouette. Um die genannte Änderung zu vermeiden wird Dilatation hier genutzt. Die Abbildung \ref{fig:eroanddila} zeigt, dass nach Erosion unerwartete Rausch außer Silhouette entfernt ist und auch die Silhouette leicht geschädigt ist. Aber nach der Dilatation wird die Silhouette besser vervollständigt. 

\begin{figure}[htpb]
	\centering
	\includegraphics[width=0.65\textwidth]{fig/eroanddila.png}
	\caption{Ergebnis nach Erosion und Dilatation} 
	\label{fig:eroanddila}
\end{figure}

\subsection{Eine Verbesserung für Hintergrundsubtraktion}
Im Allgemein wird jeder Pixel von aktuellem Bild erst berechnet, ob er zu Hintergrund oder Vordergrund gehört, und dann wird den Pixel wieder nach jedem Frame in Hintergrundmodell hinzugefügt. Das bedeutet, wenn ein Mensch sich bewegt von $A$ nach $B$ und auf $B$ so lang zum Stillstand kommt, wird der Mensch wieder in Hintergrund zugeordnet. Und das Problem muss unbedingt gelöst werden, weil die Arbeit um Erkennung einer außergewöhnlichen Situation geht, indem ein Unfall passieren kann und Person kann wahrscheinlich auf dem Boden lang liegen. Das Problem liegt an der Aktualisierung des Hintergrundmodells, deshalb wird eine Methode in diese Stelle erstellt, um das Problem zu vermeiden (Abbildungen \ref{fig:mymethod1}, \ref{fig:mymethod2}, \ref{fig:mymethod3}). Bei Aktualisierung eines Hintergrundbildes soll nur die Pixel, die nicht zu Vordergrund zugeordnet sind, betrachtet werden. Das erste Frame wird als eine Hintergrundbild in Schwarzweiß abgelagert. Der Unterschied zwischen aktuellem Bild in Schwarzweiß und dem abgelagerten Hintergrundbild wird pixelweise gerechnet.  Anschließen bekommt man ein Bild, das Unterschied zwischen aktuellem Bild und Hintergrundbild  mit Werte von 0 bis 255 darstellt  (Abbildungen \ref{fig:mymethod1} c, \ref{fig:mymethod2} c, \ref{fig:mymethod3}c). Je geringer der Wert von einem Pixel im Bild ist, ,desto höher die Wahrscheinlichkeit, dass das Pixel zu Hintergrund gehört. Und umgekehrt je höher der Wert von einem Pixel im Bild ist, desto höher die Wahrscheinlichkeit, dass das Pixel zu Hintergrund gehört. Nach der Berechnung des Unterschiedsbild wird eine Schwellwert von $50$ angewendet um ein Binärbild zu erzeugen. Wenn ein Pixel ein Wert unter $50$ hat, wird das Pixel in Schwarz markiert und höher als $50$ dann wird das Pixel in Weiß markiert (Abbildungen \ref{fig:mymethod1} d, \ref{fig:mymethod2} d, \ref{fig:mymethod3}d) .  Bis jetzt ist ein Binärbild kreiert, das ein sich bewegende Objekt enthält.  Erosion und Dilatation sind hier noch mal angewendet um Qualität des Binärbildes zu verbessern  (Abbildungen \ref{fig:mymethod1} e, \ref{fig:mymethod2} e, \ref{fig:mymethod3}e). Wie in \ref{sec:grunglagen_hintergrundsub} schon gemeint, Hintergrundsubtraktion besteht aus drei Hauptschritte: Initialisierung des Hintergrundes, Erkennung des Vordergrundes und Aktualisierung des Hintergrundes. Bis jetzt sind zwei erste Schritte schon geschafft, und es fehlt nur noch den letzte Schritte.  Ein Begrenzungsbox (eng. Bounding box) soll in dieser Stelle erstellt, um sich bewegendes Objekt zu begrenzen und auch für die Erkennung der Körperhalterung in nächstem Abschnitt zu unterstützen. Zuerst wird Gruppen von Pixel, die kontinuierlich neben einander sind, markiert. OpenCV bietet eine Funktion an, die  \inlinecode{C++}{cv2.findContours()} heißt. Die Funktion \inlinecode{C++}{findContours} nimmt ein Binärbild als Eingabe und gibt Konturen von verbundenen Pixel als Ausgabe. Mit Hilfe von der Funktion  \inlinecode{C++}{cv.contourArea()} kann man einfach die  Begrenzungsboxen herausfinden  (Abbildungen \ref{fig:mymethod1} f, \ref{fig:mymethod2} f, \ref{fig:mymethod3}f). Nun ergibt es sich eine Begrenzungsbox, die sich bewegenden Objekte begrenzt. Nach Bewegung einer Person bekommt man eine Liste von Begrenzungsbox, die alle vergangene Positionen der Bewegung enthält und eine  Begrenzungsbox, die aktuelle Bewegung darstellt. Bei Aktualisierung des Hintergrundes wird das Hintergrundbild nur mit Teil(e) des aktuellem Bildes mit vergangen Begrenzungsboxen ersetzt, die nicht die aktuelle Bewegung überschnitt . Auf den Abbildungen   \ref{fig:mymethod1} h, \ref{fig:mymethod2} h, \ref{fig:mymethod3}h werden die Teil(e) des Bildes, die mit rot markiert sind, in Hintergrundbild kopiert. Die weiß markierte Bereiche auf Abbildungen \ref{fig:mymethod1} g, \ref{fig:mymethod2} g, \ref{fig:mymethod3}g werden noch nicht beobachtet soweit wenn die noch mit dem aktuelles Begrenzungsbox (grün markiert Box) überschneiden.
\begin{figure}[htpb]
	\centering
	\def\svgwidth{1\textwidth}
	\import{./fig/}{mymethode4edited.pdf_tex}
	\caption{Frame-Nummer: 334. Die rot markierte Bereich von Hintergrundbildes wird in nächstem Frame aktualisiert.}
	\label{fig:mymethod1}
\end{figure}
\begin{figure}[htpb]
	\centering
	\def\svgwidth{1\textwidth}
	\import{./fig/}{mymethode3edited.pdf_tex}
	\caption{Frame-Nummer: 335. Nach der Aktualisierung des Hintergrundbildes wird die alle rot Begrenzungsboxen gelöscht.}
	\label{fig:mymethod2}
\end{figure}
\begin{figure}[htpb]			
	\centering
	\def\svgwidth{1\textwidth}
	\import{./fig/}{mymethode2edited.pdf_tex}
	\caption{Frame-Nummer: 403. Soweit die Person sich in der Szene bewegt, werden die vergangenen Boxen (weiß und rot) erstellt.}
	\label{fig:mymethod3}
\end{figure}

Die obengenannte Methode haben zeit Vorteile. Der erste Vorteil ist Ermöglichen, das Problem zu lösen, wenn eine Person so lang nicht genug nach Bewegungen zum Still kommt, und die Person wird als Hintergrund betrachtet. Der zweite Vorteil ist Verarbeitungszeit, da bei andere Methode in \ref{sec:grunglagen_hintergrundsub} braucht man ein Liste von Hintergrundbilder, um ein Hintergrundmodell zu bauen. Aber mit der obengenannte Verbesserung braucht man nur Hintergrundbild und aktuelles Bild.\\
Zwar funktioniert die Methode schon gut, aber man kann hier noch die Qualität von Silhouette verbessern. Man kann hier einfach bemerken, dass Silhouette von der gemeinte Methode zum Vergleich mit AGMM Verfahren noch schlecht ist. Hier kann man noch was verbessern, und das ist eine Kombination von AGMM und die obengenannt Methode. Die finale Verbesserung basiert auf die Hintergrundsubtraktion von AGMM und die Aktualisierung des Hintergrundes von meine eigene Methode. Das heißt, wenn eine Person sich bewegt in Szene bewegt, wird die Person mit einer Box markiert. Das Teil von Hintergrund, wo die Box stattfindet, wird nicht bei Aktualisierung von Hintergrund betrachtet. Der Unterschied bei Aktualisierung des Hintergrundbildes zwischen meine eigene Methode und  die Verbesserung durch AGMM ist:    
\begin{itemize}
	\item Eigene Methode: Durch eine Bewegung wird das Hintergrundbild nur an die vergangenen Begrenzungsboxen (weißen Boxen), die nicht mit aktueller Bewegung überschneidet, aktualisiert.
	\item Verbesserung mit AGMM: Durch eine Bewegung wird das ganze Hintergrundbild außer dem Bereich, wo aktuelle Bewegung stattfindet, aktualisiert.
\end{itemize}
Diese finale Verbesserung verbessert die Qualität von Silhouette und deswegen erhöht auch die Genauigkeit der Schätzung der Körperhaltung, die in nächstem Abschnitt besprochen wird.  Allerdings, die Kombination von AGMM und meine eigene Methode hat einen Nachteil, dass die Laufzeit länger als erster Versuch mit eigene Methode. Die Laufzeit betragt vorher durchschnitt 45 Millisekunden für ein Bild und nachher gegen 100 Millisekunden, aber es ist noch akzeptierbar für eine Echtzeitanwendung.

\begin{figure}[htpb]
	\centering
	\includegraphics[width=1\textwidth]{fig/dilatation.jpg}
	\caption{Bilder zum Vergleichen von eigener Methode und Kombinationsmethode} 
	\label{fig:compare}
\end{figure}

\subsection{Ergebnisse der eigene Hintergrundsubtraktion}
Um Qualität von erstellte  Silhouette zu bewerten, wurde ``Mean Squared Error"\ und ``Structural Similarity Measure" zum Einsatz gebracht. Ein ``Groundtrue"-Bild wird manuell erstellt und mit den Silhouetten von zwei Verbesserungen verglichen.  Die Formel von ``Mean Squared Error"\ wird wie folgt beschrieben:\\

\begin{equation}
MSE = \frac{1}{m n } \sum \limits_{i=0}^{m -1} \sum \limits_{j=0}^{n -1} [ I(i, j) - K (i, j)]^2
\end{equation}
wobei $m$ und $n$ sind die Breite und Höhe des Bildes sind, und $MSE$ wird als durchschnittliche Summe von quadratischem Differenz von jeden Pixel berechnet. Je kleiner $MSE$ Wert ist, desto ähnlicher sind die zweit verglichen Bilder.  ``Structural Similarity Measure" wird in ~\cite{wang2004image} vorgestellt. Die menschliche ist visuelle Wahrnehmung stark angepasst, um strukturelle Informationen aus einer Szene zu extrahieren, deswegen ist``Structural Similarity Measure" für die Qualitätsbewertung eingeführt, der auf der Degradation von Strukturinformationen basiert und wie folgt berechnet ~\cite{wang2004image}:
\begin{equation}
SSIM (x, y) = \frac{(2\mu_x \mu_y + c_1)(2\sigma_{xy} + c_2)}{(\mu_x^2 + \mu_y^2 + x_1) ({\sigma_x}^2 + {\sigma_y}^2 + c_2)}
\end{equation}
wobei $x=\{x_i | 1, 2 \dots, N\}$ und $y=\{y_i|1, 2 \dots, N\}$ und sind Positionen des NxN Fensters in jedem Bild. $\mu_x$ und $\mu_y$ sind  die Mittelwerte der Pixelintensität in der x und y Richtung.  $\sigma_x$ und $\sigma_y$ sind Varianzen,  $\sigma_{xy}$ ist Kovarianz von $x$ und $y$. $C_1$ und $C_2$ sind zwei Konstante.  SSIM ist zwischen 0 und 1 beschränkt, und je höher das Wert von SSIM, desto ähnlicher sind die zwei Bilder. Tabelle \ref{tbl:comparesihoulette} zeigt uns den Unterschied zwischen den zeit Verbesserungen. Es ist deutlich, dass die zweite Verbesserung bessere Ergebnisse liefern kann,weil ``MSE" Werte kleiner sind, und das ``SSIM" Werte größer sind.

\begin{table}[h!]
	\begin{center}
		\begin{tabular}{ c  c  c  c  }
			\toprule
			Original & Ground true & 1. Verbesserung & 2. Verbesserung \\ 
			\bottomrule
			\raisebox{-\totalheight}{\includegraphics[width=0.25\textwidth]{fig/original1.png}}
			& 
			\raisebox{-\totalheight}{\includegraphics[width=0.25\textwidth]{fig/groundtrue1.png}}
			& 
			\raisebox{-\totalheight}{\includegraphics[width=0.25\textwidth]{fig/My1.png}}
			&
			\raisebox{-\totalheight}{\includegraphics[width=0.25\textwidth]{fig/MOG1.png}}
			\\
			\\
			& & MSE:252,12 SSIM: 0,98 & MSE: 109,36 SSIM: 0,99\\
			 \bottomrule

			\raisebox{-\totalheight}{\includegraphics[width=0.25\textwidth]{fig/original2.png}}
			& 
			\raisebox{-\totalheight}{\includegraphics[width=0.25\textwidth]{fig/groundtrue2.png}}
			& 
			\raisebox{-\totalheight}{\includegraphics[width=0.25\textwidth]{fig/My2.png}}
			&
			\raisebox{-\totalheight}{\includegraphics[width=0.25\textwidth]{fig/MOG2.png}}
			\\
			\\
			& & MSE:459,49 SSIM: 0,98 & MSE: 158,21 SSIM: 0,99\\
			\bottomrule
		
			\raisebox{-\totalheight}{\includegraphics[width=0.25\textwidth]{fig/original3.png}}
			& 
			\raisebox{-\totalheight}{\includegraphics[width=0.25\textwidth]{fig/groundtrue3.png}}
			& 
			\raisebox{-\totalheight}{\includegraphics[width=0.25\textwidth]{fig/My3.png}}
			&
			\raisebox{-\totalheight}{\includegraphics[width=0.25\textwidth]{fig/MOG3.png}}
			\\
			\\
			& & MSE:268,24 SSIM: 0,99 & MSE: 121,59 SSIM: 0,99\\
			\bottomrule
			
			
			\raisebox{-\totalheight}{\includegraphics[width=0.25\textwidth]{fig/original4.png}}
			& 
			\raisebox{-\totalheight}{\includegraphics[width=0.25\textwidth]{fig/groundtrue4.png}}
			& 
			\raisebox{-\totalheight}{\includegraphics[width=0.25\textwidth]{fig/My4.png}}
			&
			\raisebox{-\totalheight}{\includegraphics[width=0.25\textwidth]{fig/MOG4.png}}
			\\
			\\
			& & MSE:479,95 SSIM: 0,97 & MSE: 111,75 SSIM: 0,99\\
			\bottomrule
			
			\raisebox{-\totalheight}{\includegraphics[width=0.25\textwidth]{fig/original5.png}}
			& 
			\raisebox{-\totalheight}{\includegraphics[width=0.25\textwidth]{fig/groundtrue5.png}}
			& 
			\raisebox{-\totalheight}{\includegraphics[width=0.25\textwidth]{fig/My5.png}}
			&
			\raisebox{-\totalheight}{\includegraphics[width=0.25\textwidth]{fig/MOG5.png}}
			\\
			\\
			& & MSE:182,61 SSIM: 0,99 & MSE: 54,42 SSIM: 1.0\\
			\bottomrule
		\end{tabular}
		\caption{Qualität der Silhouette nach zweit Verbesserungen. Je kleiner das ``MSE" Wert desto besser ist das Ergebnis. Je größer das ``SSIM" Wert desto besser ist das Ergebnis.}
		\label{tbl:comparesihoulette}
	\end{center}
\end{table}

\section{Schätzung der Körperhalterung mit Histogrammanalyse}
Im Abschnitt \ref{chp:BackgroundSubtraction} wurde eine Silhouette von einer Person durch Hintergrundsubtraktion erstellt und in diesem Abschnitt wird der zweite Schritt von meiner Arbeit beschrieben. Es geht um Schätzung der Körperhaltung mit Hilfe von Histogrammanalyse. Wie es schon im Kapitel \ref{chp:Grundlageb} gemeint, die Schätzung der Körperhaltung in meiner Arbeit basiert auf die Forschung von ~\cite{haritaoglu1998ghost}. Nach der Hintergrundsubtraktion bekommt man ein Binärbild, das Bild stellt  eine Bewegung in Szene dar. Die Bewegung wird durch eine Begrenzungsbox eingeschränkt ( (Abbildungen \ref{fig:mymethod1} f, \ref{fig:mymethod2} f, \ref{fig:mymethod3}f)). Mit Hilfe von dieser Begrenzungsboxen werden die Silhouette von dem ganzem Bild extrahieren, und man bekommt ein Subimage wo nur die Silhouette enthält.  Nach der Erzeugung von dem Subimage wird das Ergebnis zuerst auf 128 Pixel skaliert. Falls die Breite größer als Höhe des Bildes wird die Breite auf 128 Pixel skaliert und die Höher wird auch skaliert, sodass das Verhältnis zwischen Breite und Höhe nicht geändert wird. Wenn die Höhe größer als Breite des Bildes wird die Höhe analog skaliert. Nach der Skalierung werden die Histogramme von horizontale sowie vertikale Achse gerechnet.  Bei eine kürzere Ach von $x$ und $y$ wird die Histogramm in der Mitte von 128 Pixel lokalisiert und die überflüssige Positionen an der Histogramm werden mit Null erfüllt. Wie schon im Abschnitt \ref{sec:Histogrammanalyse} beschrieben, die berechnete Histogramme werden mit vordefiniert normalisierte Projektionshistogramme (siehe Abbildung \ref{fig:histogramm}) mit der Formel \ref{eq:loglikelyhood} verglichen.  Je kleiner das Ergebnis von der Formel \ref{eq:loglikelyhood} ist, desto höher die Wahrscheinlichkeit, dass die Person die Körperhaltung hat.
Das Ergebnis von Körperhaltung basiert auf die Ähnlichkeit zwischen den Histogramme mit Hilfe Loglikelihood-Funktion. Um den zweiten Hauptschritt, stellt die Abbildung \ref{fig:schatzung} konkret wie die Schätzung der Körperhaltung dar. 
\begin{figure}[htpb]
	\centering
	\def\svgwidth{0.9\textwidth}
	\import{./fig/}{schatzung.pdf_tex}
	\caption{Zweiter Hauptschritt}
	\label{fig:schatzung}
\end{figure}
Jede Körperhaltung hat einen eindeutigen Graph zum Vergleich mit anderen Körperhaltungen, deswegen ist das Ergebnis von der zweite Hauptschritt ist sehr zuverlässig. Um die Zuverlässigkeit von dieser Schritt zu überprüfen, wird Körperform im Testvideos Frame nach Frame markiert, wo die Testperson liegen, stehen, sitzen... Und das ergibt schon die Ergebnisse von mehr als $70\%$ Genauigkeit.  Die graphische Darstellung in Abbildungen \ref{fig:schatzeva} und \ref{fig:schatzeva2} zeigt uns wie gut die Schätzung der Körperhaltung ist. Die Körperhaltung wird als Nummern zugeordnet, Null seht für Stehen, Eins steht für Liegen, Zwei steht für Beugen und Drei steht für Sitzen. Wenn die Schätzung der Körperhaltung besonders minus Eins sich ergibt, dann bedeutet das keine Bewegung mehr im Szene stattfindet. Die blaue Gerade ist manuelle Markierung in Testvideos und die gelbe Gerade ist das Ergebnis von unserem Programm. Anhand der graphische Darstellungen (Abbildung \ref{fig:schatzeva} und \ref{fig:schatzeva2}) kann man einfach sehen, dass in manche Stelle wird die Schätzung in eine andere Körperhaltung gesprungen ist und dann kam wieder in die richtige Position. Das Problem kann man einfach mit Hilfe von Entfernung des Rausches in Graph (wie in ~\cite{guo2006projection}) anwenden, dann kann man noch besseres Ergebnis bekommen. Da der Fokus von meiner Arbeit ist nicht speziell an Erkennung der Körperhaltung, sondern an Erkennung der außergewöhnliche Situationen, deshalb ist das Problem mit dem Rausch im Graph akzeptierbar.  

\begin{figure}[htpb]
	\centering
	\includegraphics[width=0.9\textwidth]{fig/schatzungevaluation.png}
	\caption{Überprüfung der Schätzung der Körperhaltung an Testvideo 1} 
	\label{fig:schatzeva}
\end{figure}

\begin{figure}[htpb]
	\centering
	\includegraphics[width=0.9\textwidth]{fig/schatzungevaluation2.png}
	\caption{Überprüfung der Schätzung der Körperhaltung an Testvideo 2} 
	\label{fig:schatzeva2}
\end{figure}

\section{Erkennung außergewöhnlicher Situation}	