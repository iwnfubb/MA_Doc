\chapter{Eigenes Verfahren}\label{chp:EigVerfahren}
Diese Arbeit besteht aus drei große Meilensteine. Das Hintergrundsubtraktion-Verfahren, Schätzung der Körperhaltung mithilfe von einer Histogrammanalyse und die Erkennung von außergewöhnlichen Situationen mithilfe der Fuzzylogik. Die Abbildung \ref{fig:allgemein} veranschaulicht die erwähnten Meilensteine nochmals.\\
Die hier verwendeten Testvideos haben eine Auflösung von 1280x720 Pixel und werden mit 15 Frames pro Sekunde aufgenommen. Der erste Meilenstein besteht aus der Hintergrundsubtraktion, und wird in Blau dargestellt. Die Eingabe für eine Hintergrundsubtraktion ist ein von der Kamera aufgenommenes Bild und die Ausgabe ist eine Silhouette einer Person. Der zweite Meilenstein ist die Histogrammanalyse, die zur Erkennung einer Körperhaltung zum Einsatz kommt, und ist der gelbe Teil der Abbildung . Die Eingabe dieses Meilensteins ist die Silhouette aus dem vorherigen Schritt und die Ausgabe ist die Schätzung der Körperhaltung der Person auf der >Silhouette. Der letzte Meilenstein ist die Fuzzylogik, die mit der roten Farbe dargestellt ist. Die Eingaben der Fuzzylogik sind die Schätzung der Körperhaltung, die Position der Person und die Zeit. Die Aufgabe der Fuzzylogik ist es, eine Entscheidung zu treffen, ob es sich um eine außergewöhnliche Situation handelt oder nicht.
\begin{figure}[H]			
	\centering
	\includegraphics[width=0.65\textwidth]{fig/allgemein.pdf}
	\caption{Die drei Meilensteine zur Erkennung von außergewöhnlichen Situation.}
	\label{fig:allgemein}
\end{figure}

\section{Die Hintergrundsubtraktion}\label{chp:BackgroundSubtraction}
\subsection{Vergleich verschiedener Methoden}
Die Abbildung \ref{fig:bgsub} zeigt allgemein die Grundfunktion der Hintergrundsubtraktion. Die Aufgaben dieses Verfahrens ist es, von der Bildeingabe den statischen Teil des Bildes zu extrahieren bzw. den Hintergrund korrekt zu filtern und in ein Modell zu speisen. Außerdem soll dieses Hintergrundmodell immer aktuell bleiben, so wird je Eingabe Frame/Bild das Hintergrundmodell aktualisiert.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{fig/bgsub.pdf}
	\caption{Veranschaulichung des Hintergrundsubtraktion-Verfahren.}
	\label{fig:bgsub}
\end{figure}
Es existieren mehrere Verfahren, um eine Hintergrundsubtraktion durchzuführen.
Um ein für diese Arbeit geeignetes Verfahren zu finden, wurden zuerst mehrere Verfahren verglichen.  Im Abschnitt \ref{sec:grunglagen_hintergrundsub} (Gaussian Mixture Model, K-nächste Nachbar, Kernel Density Estimation und Vibe) wurden die bekanntesten Verfahren theoretisch beschrieben. Hier werden die einzelnen Verfahren so geprüft, um herauszufinden, ob sie für den Echtzeiteinsatz geeignet sind oder doch nicht. Die für diese Arbeit eingesetzte Kamera nimmt bis zu 15 Frames die Sekunde auf. Des weiteren beträgt die hier verwendete Rechenleistung 13.75 GFlops. Ziel ist es, ein Verfahren auszuwählen, dass 15 Frames pro Sekunde mit der eingesetzten Rechenleistung abarbeiten kann. Die Rechenleistung gilt nicht als besonders Leistungsfähig. Bei \acs{AGMM} wird eine Mischung von 5 Gaußschen Verteilungen für das Hintergrundmodell verwendet und die Historie des Hintergrundmodells auf 100 Bilder limitiert. Die Lernrate wird auf $0.01$ gesetzt. Für das \acs{KNN} Verfahren wurden auch die Parameter mit den gleichen Werten belegt, um den Vergleich der Hintergrundsubtraktionsverfahren objektiv bewerten zu können. Bei \acs{KDE} handelt es sich um eine Berechnung von Intensitätswerten für jeden Pixel, daher wird nur der Parameter der \grqq{}Historie\grqq{} (wie bei \acs{AGMM}) belegt. Wie in \cite{barnich2009vibe} beschrieben, ist Vibe ein nicht-parametrisches Verfahren, deshalb musste kein Parameter konfiguriert werden.
In der folgenden Abbildung werden die Vergleiche der vier Verfahren veranschaulicht.

\begin{table}[H]
	\begin{center}
		\begin{tabular}{ | c  | c  | }
			\hline
			\raisebox{-\totalheight}{\includegraphics[width=0.5\textwidth]{fig/motiondetrection_AGMM.png}}
			& 
			\raisebox{-\totalheight}{\includegraphics[width=0.5\textwidth]{fig/motiondetrection_KNN.png}}
			\\
			1. AGMM & 2. KNN\\
			\hline
			
			\raisebox{-\totalheight}{\includegraphics[width=0.5\textwidth]{fig/motiondetrection_KDE.png}}
			& 
			\raisebox{-\totalheight}{\includegraphics[width=0.5\textwidth]{fig/motiondetrection_Vibe.png}}
			\\
			3. KDE & 4. Vibe\\
			\hline
		\end{tabular}
		\caption{Vergleiche von 1. AGMM 2. KNN 3. KDE 4. Vibe}
		\label{fig:compare_bgsubtraction}
	\end{center}
\end{table}
Aus der obigen Grafik wird deutlich, dass sich in den Verfahren AGMM und KNN viel Rausch im Resultat der Hintergrundsubtraktion befindet. Die Resultate sind daher als schlecht zu bewerten. Die Verfahren KDE und Vibe liefern sehr gut Resultate bzw. eine Silhouette mit weniger Rausch, jedoch benötigt das Vibe-Verfahren deutlich mehr Zeit. Somit ist das Verfahren KNN, verglichen mit den anderen, das Verfahren, dass eine klare Silhouette mit weniger Rausch in kürzester Zeit liefert. 
Anhand des Schaubildes ist das \acs{KNN}-Verfahren ein geeigneter Kandidat für die Echtzeitanwendung. Nun muss noch geprüft werden, ob sich das Verfahren für bis zu 15 Frames einsetzten lässt. Aus dem Test war nur zu erkennen, dass es in erster Linie ein valides Resultat liefert. Zweitrangig wurden die Verfahren nach der Zeit bewertet. Aus dem Test ist nicht ganz ersichtlich, ob sich das Verfahren denn im Dauerbetrieb anwenden lässt. Um das zu prüfen, wurden viele Testvideos mit Tageslicht und andere abends mit häuslicher Beleuchtung aufgenommen und analysiert. Ziel ist es, dass die Echtzeitanwendung stabile Resultate bei allen Beleuchtungen liefert.

Aus der Tabelle \ref{fig:compare_bgsubtraction} mit vier Abbildungen ist zu sehen, dass die Verfahren Vibe und KNN deutlich bessere (weniger Rauschen) Resultate liefern. Um aus den beiden Verfahren eins auszuwählen, wird eine Laufzeitanalyse bestimmt. Die Abbildung \ref{fig:vibeknn} stellt die Laufzeitanalyse der Verfahren KNN und Vibe mit einem Rechner, der mit einem Intel i5 3,20 GHz ausgestattet ist, dar.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{fig/knn_vs_vibe_edit.pdf}
	\caption{Die Verarbeitungszeit von KNN und Vibe werden verglichen. Es ist deutlich, dass KNN eine kürzere durchschnittliche Verarbeitungszeit (42ms) als Vibe (165ms) hat.} 
	\label{fig:vibeknn}
\end{figure}

Aus der obigen Laufzeitanalyse ist zu entnehmen, dass das KNN-Verfahren deutlich effizienter ist als das Vibe-Verfahren. Aus den Gründen, dass das KNN-Verfahren gute Resultate in Bestzeit im Vergleich liefert, wird für die Echtzeitanwendung dieser Arbeit auch dieses Verfahren eingesetzt. Die oben genannten Hintergrundsubtraktionsverfahren basieren auf der Änderung der Intensitätswerten jedes Pixel, deshalb liefern die Verfahren ein Binärbild mit Rausch zurück. Um das Rauschen zu entfernen, kommen Erosion und Dilatation zum Einsatz. Die zwei Verfahren sind die grundlegenden Operationen bei der morphologischen Bildverarbeitung, auf der alle anderen morphologischen Operationen basieren. Um das Rauschen zu entfernen, wird das Verfahren Öffnung verwendet. Der Begriff Öffnung bedeutet eine Schachtelung von den Verfahren Erosion und Dilatation hintereinander.

Die Erosion ist ein Verfahren, dass ein Bild mit einem vordefinierten Filter untersucht und daraus Schlussfolgerungen zieht, wie dieser Filter in die Formen im Bild passt. Wenn ein Pixel und seine Nachbarpixel vollständig von dem Filter überlagert werden, wird das Pixel nicht verändert, sonst wird das Pixel gelöscht. Gelöscht wird indem das Pixel schwarz wird. Angenommen ist $A$ ein Binärbild und $B$ ein Filter mit einem Zentrum in der Mittel von $B$. Wenn $B$ sich in $A$ bewegt, überlagert für jedes Pixel in $A$ der Ursprung von $B$. Wenn $B$ vollständig in $A$ enthalten ist, wird das Pixel bei der Erosion beibehalten, ansonsten gelöscht\cite{zamperoni2013methoden}. Die folgende Grafik veranschaulicht die Erosion.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{fig/erosion.pdf}
	\caption{Die Erosion mit einem 3x3 Filter.} 
	\label{fig:erosion}
\end{figure}
Die Dilatation ist ein gegenteiliges Verfahren zur Erosion, es versucht die Form eines Bildes zu vergrößern. Wenn $B$ sich in $A$ bewegt, überlagert für jedes Pixel in $A$ der Ursprung von $B$. Alle Pixel in $A$, die vollständig von $B$ überlagert sind, werden beibehalten oder markiert \cite{zamperoni2013methoden}.\\
Die folgende Grafik veranschaulicht die Dilatation.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{fig/dilatation.pdf}
	\caption{Die Dilatation mit einem 3x3 Filter.} 
	\label{fig:dilatation}
\end{figure}
Um den Rausch aus der Silhouette zu entfernen wird das Öffnen-Verfahren angewandt. Auf die Silhouette wird zuerst eine Erosion durchgeführt und anschließend eine Dilatation. Durch diese Maßnahme wird die Silhouette zuerst scharfer bzw. alle Punkte, die nicht zur Person auf der Silhouette gehören, gelöscht. Allerdings kann es passieren, dass die Punkte die zur Person gehören etwas undichter werden. Um diesen Fehler zu verbessern und die Silhouette wieder auf die original Größe zurück zu bekommen, wird die Dilatation angewendet. So bleibt am Ende die Silhouette gleich große und ohne Rausch.\\
Die Abbildung \ref{fig:eroanddila} zeigt, dass nach Erosion unerwartetes Rauschen außer Silhouette entfernt ist und auch die Silhouette kleiner wird. Um dies zu kompensieren wird eine Dilatation verwendet. 
\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\textwidth]{fig/eroanddila.pdf}
	\caption{Ein Beispiel für die Öffnung (Erosion und Dilatation).}
	\label{fig:eroanddila}
\end{figure}
\subsection{Erweiterungen für die Hintergrundsubtraktion}
Im Allgemeinen wird für jedes Pixel des aktuellen Bilds geschaut, ob es dem Hintergrund zugehörig ist. Das funktioniert wie beschrieben sehr zuverlässig. Nun wird folgende Situation betrachtet: Eine Person befindet sich eine gewisse Zeit im Raum und bewegt sich nicht. Durch die ständige Aktualisierung des Hintergrundmodells wird die Person Fehlerhaft als Hintergrund erkannt und verschwindet in das Hintergrundmodell. So kommt die Aufgabe dieser Echtzeitanwendung nicht zum Einsatz, weil bei einer Außergewöhnlichen Situation muss sich eine Person eine gewisse Zeit nicht mehr bewegen bzw. auf dem Boden liegen. Aber dieser Fall wird nie auftreten. Auch Situationen wo die Person zum Beispiel auf dem Sofa liegt und TV sieht, befindet sich die Person nicht in Gefahr und auch nicht im Hintergrund. 
\subsubsection{Erste Verbesserung: Aktualisierung der selektiven Begrenzungsboxen}
Um das oben genannte Problem zu lösen, wo eine sich nicht bewegende Person in das Hintergrundmodell integriert wird, obwohl die Person in den Vordergrund bleiben soll, wurde ein eigenes Verfahren entwickelt. Dieses Verfahren erstellt bei der Verarbeitung der Frames Begrenzungsboxen in Form von Rechtecken zur bewegten Person. Diese Begrenzungsboxen sind Minimal bzw. umhüllen die Person mit dem kleinst möglichen Flächen. Das heißt, auch wenn die Person sich lange nicht bewegt, bleibt sie ein Objekt, dass sich bewegt und wird nicht in das Hintergrundmodell integriert. Das geschieht durch eine geführte Liste der Begrenzungsboxen. Diese Liste hat die gleiche Länge wie die gesetzte Historie des Hintergrundsubtraktionsverfahren. So wird bei jedem zu verarbeitenden Frame eine Begrenzungsbox aus der Silhouette erstellt und mit der Liste aus vorherigen Begrenzungsboxen verglichen. Falls die neue Begrenzungsbox mit den vorherigen Begrenzungsboxen keine Überschneidungen hat, werden diese Begrenzungsboxen in das Hintergrundmodell hinzugefügt und aus der Liste entfernt. Die neue Begrenzungsbox wird immer der Liste hinzugefügt, egal ob es Überschneidungen gibt oder nicht. So können Personen die sich im Raum befinden und sich dann auf das Sofa legen und sich ausruhen, bleiben im Vordergrund bis sie die Szene verlassen. Wie das modifizierte Hintergrundsubtraktionsverfahren, genannt \glqq{}Aktualisierung der selektiven Begrenzungsboxen\grqq{} (ASB), arbeitet, zeigt folgende Grafiken \ref{fig:mymethod1}, \ref{fig:mymethod2}, \ref{fig:mymethod3}.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\textwidth]{fig/mymethode4edited.pdf}
	\caption{Video: \glqq{}fallen.mp4\grqq{}. Frame-Nummer: 334. Die Begrenzungsbox $B_i$, die mit einem roten Rechteck markiert ist, überschneidet sich mit der aktuellen Begrenzungsbox $B_{current}$, die grün markiert ist. Das Unterbild in dem roten Rechteck wird auf dem Hintergrundbild aktualisiert.}
	\label{fig:mymethod1}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\textwidth]{fig/mymethode3edited.pdf}
	\caption{Video: \glqq{}fallen.mp4\grqq{}. Frame-Nummer: 335. Nach der Aktualisierung des Hintergrundbildes wird die alte Begrenzungsbox $B_i$ gelöscht.}
	\label{fig:mymethod2}
\end{figure}
\begin{figure}[H]			
	\centering
	\includegraphics[width=0.9\textwidth]{fig/mymethode2edited.pdf}
	\caption{Video:\glqq{}fallen.mp4\grqq{}. Frame-Nummer: 403. Die Liste der Begrenzungsboxen, die sich nicht mit der aktuellen Begrenzungsbox $B_{current}$ überschneiden, werden mit den weißen Rechtecken dargestellt. Die Liste der Begrenzungsboxen, die sich mit der aktuellen Begrenzungsbox $B_{current}$  überschneiden, werden mit den roten Rechtecken dargestellt. 
	}
	\label{fig:mymethod3}
\end{figure}
Bei der Aktualisierung eines Hintergrundbildes, wo noch kein Hintergrundmodell aufgebaut ist, wird das gesamte Bild als Hintergrund betrachtet. Sonst sollen nur die Pixel, die nicht zum Vordergrund zugeordnet sind, betrachtet werden. Das  wird in ein Grauwertbild mit der Originalgröße (1280x720) und mit dem Wertbereich $[0,255]$ konvertiert und als ein Hintergrundbild $H$ gespeichert. Der Unterschied zwischen dem aktuellen Grauwertbild $I$ und dem abgespeicherten Hintergrundbild $H$ wird pixelweise mit absoluter Subtraktion gerechnet. Anschließend ergibt sich ein Bild $S$, das den Unterschied zwischen dem aktuellen Bild und dem Hintergrundbild  mit den Werten von 0 bis 255 darstellt  (siehe Abbildungen \ref{fig:mymethod1}c, \ref{fig:mymethod2}c, \ref{fig:mymethod3}c). Je geringer der Wert eines Pixels auf dem Bild $S$ ist, desto höher die Wahrscheinlichkeit, dass der Pixel zum Hintergrund gehört. Und umgekehrt je höher der Wert eines Pixel auf dem Bild $S$ ist, desto höher ist die Wahrscheinlichkeit, dass das Pixel zum Hintergrund gehört. Nach der Berechnung des Unterschiedsbild wird ein Schwellenwert von $50$ angewendet, um ein Binärbild zu erzeugen. Alle Pixel mit Werten unter $50$, werden schwarz markiert und alle anderen Weiß (Abbildungen \ref{fig:mymethod1}d, \ref{fig:mymethod2}d, \ref{fig:mymethod3}d).
\\
Der nächste Schritt ist die Anwendung des Öffnung-Verfahren, um den Rausch der erstellten Silhouette zu entfernen (siehe Abbildungen \ref{fig:mymethod1}e, \ref{fig:mymethod2}e, \ref{fig:mymethod3}e).
Eine Begrenzungsbox (eng. Bounding box) wird an dieser Stelle erstellt, um das sich bewegende Objekt zu begrenzen. Diese Begrenzungsbox wird für die Erkennung der Körperhaltung im nächsten Abschnitt genutzt.\\
Des weiteren werden die Konturen der Silhouette mithilfe der Methode \inlinecode{C++}{findContours()} von Open-CV  berechnet. Der Aufruf der Methode liefert ein Array von Arrays, wo sich die gruppierten Pixel jeweils zusammen in einem Array befinden. Eine Gruppe von Pixels bildet die Bildpunkte einer der Konturen. Für die Konturen werden mithilfe der Methode  \inlinecode{C++}{contourArea()} dann Begrenzungsboxen berechnet. Die Person befindet sich in einer der Begrenzungsboxen. Um die Person zu lokalisieren, wird die Begrenzungsbox mit der größten Fläche ausgewählt. Jetzt kann das Hintergrundmodell, mithilfe der Begrenzungsbox $B_{alktuell}$ der Person aktualisiert werden. Abbildungen \ref{fig:mymethod1}f, \ref{fig:mymethod2}f, \ref{fig:mymethod3}f) verdeutlichen diese Schritte. Wenn sich die Person von $P_1$ nach $P_2$ bewegt, wird eine Liste der vergangen Begrenzungsboxen $L_B = \{B_1, B_2, ... , B_n\}$ aktualisiert. Die Liste von Begrenzungsboxen der Person gehört zur Historie zu. Bei Aktualisierung des Hintergrundes wird die aktuelle Begrenzungsbox $B_{current}$ mit allen Begrenzungsboxen der Liste $L_B$ verglichen. Wenn eine Begrenzungsbox $B_i$ in $L_B$ mit der aktuellen Begrenzungsbox $B_{current}$ die leer Menge von Schnittpunkten liefert, wird das Unterbild in $B_i$ von dem aktuellem Bild $I$ in das Hintergrundbild $H$ an der gleichen Position ersetzt und in eine Liste $L^*_B$ verschoben. Auf den Abbildungen \ref{fig:mymethod1}h, \ref{fig:mymethod2}h, \ref{fig:mymethod3}h werden die Unterbilder mit Begrenzungsboxen in $L^*_B$, die mit rot markiert sind, in das Hintergrundbild ersetzt. Die Liste $\hat{L}_B = L_B \backslash L^*_B$ enthält alle vergangenen Begrenzungsboxen, die keine Schnittpunkte mit der aktuellen Begrenzungsbox $B_{current}$ haben. Die Liste $\hat{L}_B$ wird auf den Abbildungen \ref{fig:mymethod1}g, \ref{fig:mymethod2}g, \ref{fig:mymethod3}g mit weißer Farbe dargestellt. Außerdem wird die Begrenzungsbox $B_{current}$ mit grüner Farbe auf den Abbildungen \ref{fig:mymethod1}f, \ref{fig:mymethod2}f, \ref{fig:mymethod3}f gezeigt. Nach jeder Aktualisierung des Hintergrundbildes werden alle Elemente der Liste $L^*_B$ gelöscht und die Liste $L_B$ enthält nur die Begrenzungsboxen, die mit der aktuellen Begrenzungsbox $B_{alktuell}$ nicht überschneiden. \\
\subsubsection{Zweite Verbesserung (KNN-Plus-ASB)}
Die ASB Methode hat zwei Vorteile. Der erste Vorteil ist, dass die Methode eine Lösung des Problems ist, wenn eine Person beispielsweise stillsteht. Nach einer bestimmten Zeit wird die Person fehlerhaft in das Hintergrundmodell integriert und so nicht mehr als der Vordergrund erkannt. Der zweite Vorteil ist eine Verbesserung der Verarbeitungszeit, da bei der anderen Methode aus Kapitel \ref{sec:grunglagen_hintergrundsub} eine Liste von Hintergrundbildern gebraucht wird, um das Hintergrundmodell aufzubauen. Aber mit der obengenannten Verbesserung sind nur das Hintergrundbild und das aktuelle Bild nötig. So wird die benötigte Rechenleistung als auch Speicherverbrauch reduziert.\\
Da das Hintergrundmodell nur an Teile des aktuellen Bildes aktualisiert wird, werden kleine Änderungen am Hintergrundbild nicht betrachtet. Daher entsteht viel Rausch im Binärbild nach der Hintergrundsubtraktion. Die Aktualisierungen am Hintergundmodell werden nur unter Bedingungen durchgeführt. Diese Bedingungen sind dann erfühlt, wenn sich die Person im Raum bewegt, sonst werden keine Aktualisierungen durchgeführt. Im allgemeinen macht die ASB-Methode folgendes: Durch eine Bewegung wird das Hintergrundbild nur an den alten Begrenzungsboxen (weißen Boxen in Abbildungen \ref{fig:mymethod1} g, \ref{fig:mymethod2} g, \ref{fig:mymethod3}g), die nicht mit aktueller Bewegung überschneiden, aktualisiert. Die KNN-Plus-ASB-Methode: Ein Binärbild wird mit der \acs{KNN} Methode erstellt. Aus dem Binärbild wird eine Silhouette mit einer Begrenzungsbox beschränkt. Das Hintergrundbild außer der Bereich der Begrenzungsbox wird aktualisiert.
Die beiden genannten Verbesserungen werden im nächsten Abschnitt verglichen.
\newpage
\subsection{Ergebnisse der Verbesserungen am Hintergrundsubtraktion-Verfahren}
Um die Qualität der Silhouette bewerten zu können, wird die Silhouette mit dem Ground-Truth verglichen, was eine Silhouette ohne Rausch bildet. Das Ground-Truth-Modell wurde manuell erstellt, um sicherzustellen, dass sich kein Rausch in der Silhouette befindet. Um die Vergleiche durchzuführen, werden die Methoden \glqq{}Mean Squared Error\grqq{} und \glqq{}Structural Similarity Measure\grqq{} verwendet. Die erste Vergleich ist die (ASB-Methode). Der zweite Vergleich die KNN-Plus-ASB-Methode. 
Die Formel von \glqq{}Mean Squared Error\grqq{} wird wie folgt beschrieben\cite{kapadia2017mathematical}:\\
\begin{equation}
MSE = \frac{1}{m n } \sum \limits_{i=0}^{m -1} \sum \limits_{j=0}^{n -1} [ I(i, j) - K (i, j)]^2
\end{equation}
wobei $m$ und $n$ die Breite und Höhe des Bildes sind und $MSE$ die durchschnittliche Summe der quadratischen Differenz aller Pixel bildet. Je kleiner der $MSE$ Wert ist, desto ähnlicher sind die verglichenen Bilder.  \glqq{}Structural Similarity Measure\grqq{} wird in \cite{wang2004image} vorgestellt. Um strukturelle Veränderungen zu erkennen, kommt \glqq{}Structural Similarity Measure\grqq{} zur Qualitätsbewertung dazu und wird wie folgt berechnet\cite{wang2004image}:
\begin{equation}
SSIM (x, y) = \frac{(2\mu_x \mu_y + c_1)(2\sigma_{xy} + c_2)}{(\mu_x^2 + \mu_y^2 + x_1) ({\sigma_x}^2 + {\sigma_y}^2 + c_2)}
\end{equation}
wobei $x=\{x_i | 1, 2 \dots, N\}$ und $y=\{y_i|1, 2 \dots, N\}$ die Positionen des NxN Fensters in jedem Bild beschreibt. $\mu_x$ und $\mu_y$ sind  die Mittelwerte der Pixelintensität in x und y Richtung.  $\sigma_x$ und $\sigma_y$ sind Varianzen,  $\sigma_{xy}$ ist Kovarianz von $x$ und $y$. $C_1$ und $C_2$ sind zwei Konstante.  SSIM ist zwischen 0 und 1 beschränkt und je höher der Wert von SSIM ist, desto ähnlicher sind die zwei Bilder. Tabelle \ref{tbl:comparesihoulette} zeigt uns den Unterschied zwischen dem ASB und KNN-Plus-ASB. Es ist deutlich zu erkennen, dass die zweite Verbesserung bessere Ergebnisse liefert, weil der \glqq{}MSE\grqq{} Wert kleiner und der \glqq{}SSIM\grqq{} Wert größer ist.
\begin{table}[H]
	\begin{center}
		\begin{tabular}{ c  c  c  c  }
			\toprule
			Original & Ground Truth & 1. Verbesserung & 2. Verbesserung \\ 
			\bottomrule
			\raisebox{-\totalheight}{\includegraphics[width=0.25\textwidth]{fig/original1.png}}
			& 
			\raisebox{-\totalheight}{\includegraphics[width=0.25\textwidth]{fig/groundtrue1.png}}
			& 
			\raisebox{-\totalheight}{\includegraphics[width=0.25\textwidth]{fig/My1.png}}
			&
			\raisebox{-\totalheight}{\includegraphics[width=0.25\textwidth]{fig/MOG1.png}}
			\\
			\\
			& & MSE:252,12 SSIM: 0,98 & MSE: 109,36 SSIM: 0,99\\
			 \bottomrule

			\raisebox{-\totalheight}{\includegraphics[width=0.25\textwidth]{fig/original2.png}}
			& 
			\raisebox{-\totalheight}{\includegraphics[width=0.25\textwidth]{fig/groundtrue2.png}}
			& 
			\raisebox{-\totalheight}{\includegraphics[width=0.25\textwidth]{fig/My2.png}}
			&
			\raisebox{-\totalheight}{\includegraphics[width=0.25\textwidth]{fig/MOG2.png}}
			\\
			\\
			& & MSE:459,49 SSIM: 0,98 & MSE: 158,21 SSIM: 0,99\\
			\bottomrule
		
			\raisebox{-\totalheight}{\includegraphics[width=0.25\textwidth]{fig/original3.png}}
			& 
			\raisebox{-\totalheight}{\includegraphics[width=0.25\textwidth]{fig/groundtrue3.png}}
			& 
			\raisebox{-\totalheight}{\includegraphics[width=0.25\textwidth]{fig/My3.png}}
			&
			\raisebox{-\totalheight}{\includegraphics[width=0.25\textwidth]{fig/MOG3.png}}
			\\
			\\
			& & MSE:268,24 SSIM: 0,99 & MSE: 121,59 SSIM: 0,99\\
			\bottomrule
			
			
			\raisebox{-\totalheight}{\includegraphics[width=0.25\textwidth]{fig/original4.png}}
			& 
			\raisebox{-\totalheight}{\includegraphics[width=0.25\textwidth]{fig/groundtrue4.png}}
			& 
			\raisebox{-\totalheight}{\includegraphics[width=0.25\textwidth]{fig/My4.png}}
			&
			\raisebox{-\totalheight}{\includegraphics[width=0.25\textwidth]{fig/MOG4.png}}
			\\
			\\
			& & MSE:479,95 SSIM: 0,97 & MSE: 111,75 SSIM: 0,99\\
			\bottomrule
			
			\raisebox{-\totalheight}{\includegraphics[width=0.25\textwidth]{fig/original5.png}}
			& 
			\raisebox{-\totalheight}{\includegraphics[width=0.25\textwidth]{fig/groundtrue5.png}}
			& 
			\raisebox{-\totalheight}{\includegraphics[width=0.25\textwidth]{fig/My5.png}}
			&
			\raisebox{-\totalheight}{\includegraphics[width=0.25\textwidth]{fig/MOG5.png}}
			\\
			\\
			& & MSE:182,61 SSIM: 0,99 & MSE: 54,42 SSIM: 1.0\\
			\bottomrule
		\end{tabular}
		\caption{Vergleiche von ASB und KNN-Plus-ASB. Je kleiner der \glqq{}MSE\grqq{} Wert ist, desto besser ist das Ergebnis. Je größer der \glqq{}SSIM\grqq{} Wert ist, desto besser ist das Ergebnis.}
		\label{tbl:comparesihoulette}
	\end{center}
\end{table}
Diese KNN-Plus-ASB verbessert die Qualität der Silhouette und erhöht auch die Genauigkeit der Schätzung der Körperhaltung, die in nächstem Abschnitt besprochen wird.  Allerdings, ist die Kombination von \acs{KNN} und \acs{ASB} hat einen Nachteil, dass die Laufzeit sich etwas erhört, im Vergleich mit der erster Methode.
Zuletzt kommt noch eine kurz Laufzeitanalyse, um sicherzustellen, dass die Verbesserungen einsetzbar sind und nicht zu hohe Rechenzeit benötigen. Aus der Abbildung xyz ist zu entnehmen, dass die Verbesserung ABS eine Laufzeit von xx ms aufweist, wobei KNN-Plus-ASB yy ms benötigt.
\section{Schätzung der Körperhaltung mittels Histogrammanalyse}\label{chp:schatzung}
Im Abschnitt \ref{chp:BackgroundSubtraction} wurde eine Silhouette von einer Person durch Hintergrundsubtraktion erstellt. In diesem Abschnitt wird der zweite Meilenstein dieser Arbeit beschrieben. Es geht um die Schätzung der Körperhaltung mit Hilfe von Histogrammanalyse. Wie schon im Kapitel \ref{chp:Grundlageb} beschrieben, basiert die Schätzung der Körperhaltung auf die Forschung von Haritaoglu, Hatwood und Davis in \cite{haritaoglu1998ghost}. Die Histogrammanalyse zur Schätzung der Körperhaltung in dem sogenannten \glqq{}Ghost\grqq{}-System in \cite{haritaoglu1998ghost} wird hier angewendet. Nach der Hintergrundsubtraktion wird ein Binärbild erzeugt, welches eine Bewegung in der Szene darstellt. Die Bewegung wird durch eine Begrenzungsbox eingeschränkt (siehe Abbildungen \ref{fig:mymethod1}f, \ref{fig:mymethod2}f, \ref{fig:mymethod3}f). Mit Hilfe von dieser Begrenzungsboxen werden die Silhouetten aus dem Bild extrahiert und es wird ein Unterbild erstellt, das nur die Silhouette enthält.
Um bei der Histogrammanalyse vergleichbare Ergebnisse für unterschiedlich große Silhouetten zu bekommen, muss eine Skalierung vorgenommen werden. Nach der Erzeugung von dem Unterbild wird das Ergebnis zuerst auf 128 Pixel skaliert. Dabei wird das Verhältnis von Höhe und Breite nicht verändert. Falls das skalierte Bild auf einer Achse kleiner als 128 Pixel geworden ist, dann wird der Rest der verbliebenen Pixel mit Nullen aufgefüllt, wobei das skalierte Bild in der Mitte bleibt (siehe Abbildung \ref{fig:histogramverschiebung}). Das Unterbild wird also immer auf beiden Achsen zentriert mit dem Referenz-Histogramm verglichen.\\
\begin{figure}[H]
	\centering	
	\includegraphics[width=1\textwidth]{fig/histogramverschiebung.pdf}
	\caption{Wenn die Länge eines Histogramms kleiner als 128 Pixel ist, wird das Histogramm auf einem 128 Pixel lang Histogramm abgebildet. Die Werte des originalen Histogramms werden in der Mitte des neuen Histogramm zentriert.}
	\label{fig:histogramverschiebung}
\end{figure}

Wie schon im Abschnitt \ref{sec:Histogrammanalyse} beschrieben, können mit der Formel \ref{eq:loglikelyhood} die berechneten Histogramme mit den vordefinierten normalisierten Referenz-Histogrammen (siehe Abbildung \ref{fig:histogramm}) verglichen werden. Je größer das Ergebnis aus der Formel \ref{eq:loglikelyhood} ist, desto höher ist die Wahrscheinlichkeit, dass die Person die betrachtete Körperhaltung annimmt. Die Abbildung \ref{fig:schatzung} zeigt, wie sich aus dem zweiten Meilenstein die Schätzung der Körperhaltung zu Stande kommt. Die Abbildungen \ref{fig:schatzung1}, \ref{fig:schatzung2} und \ref{fig:schatzung3} repräsentieren die Schätzung der Körperhaltung in graphische Beispiele, um den zweiten Schritt klarzustellen.
\begin{figure}[H]
	\centering	
	\includegraphics[width=0.95\textwidth]{fig/schatzung.pdf}
	\caption{Der zweite Meilenstein ist die Schätzung der Körperhaltung. Das Ausgabebinärbild des ersten Meilensteins wird mit einer Begrenzungsbox beschränkt, um ein Unterbild von der Silhouette zu erstellen. Das Unterbild wird dann skaliert und die vertikalen und horizontalen Histogramme des skalierten Unterbildes werden berechnet. Diese Histogramme werden mit den Referenz-Histogrammen verglichen, um die Körperhaltung auf dem Unterbild zu bestimmen.}
	\label{fig:schatzung}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{fig/schatzung1.pdf}
	\caption{Ein Beispiel für einen Prozess von Schätzung einer Silhouette beim Stehen. Die Silhouette des Binärbildes wird zuerst durch eine Begrenzungsbox extrahiert. Die vertikalen und horizontalen Histogramme der Silhouette werden dann berechnet und mit den entsprechenden Referenz-Histogrammen jeder generischen Körperhaltung verglichen. Die Körperhaltung, die nach dem Vergleich den größten Wert hat, wird für die Silhouette genommen.}
	\label{fig:schatzung1}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{fig/schatzung2.pdf}
	\caption{Beispiel für einen Prozess von Schätzung einer Silhouette beim Liegen.}
	\label{fig:schatzung2}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{fig/schatzung3.pdf}
	\caption{Beispiel für einen Prozess von Schätzung einer Silhouette bei Beugen.}
	\label{fig:schatzung3}
\end{figure}


Um die Zuverlässigkeit von diesem Verfahren zu überprüfen, wird die Körperhaltung in Testvideos Bild für Bild annotiert, auf welchem Bild und in welcher Position in der Szene die Testperson liegt, steht, sitzt oder sich beugt. Das ergibt eine Genauigkeit von mehr als $70\%$. Die graphische Darstellung in Abbildungen \ref{fig:schatzeva} und \ref{fig:schatzeva2} zeigt das Ergebnis der Schätzung der Körperhaltung. Dabei wird die Körperhaltung als Nummer kodiert. Null steht für stehend, eins für liegend, zwei für beugend und drei steht für sitzend. Wenn die Schätzung minus eins ergibt, dann bedeutet das, dass sich keine sich bewegende Objekte (in diesem Fall Personen) mehr in der Szene befinden. Die blaue Gerade stellt den Referenz-Wert in den Testvideos dar und die gelbe Gerade ist das Ergebnis der Schätzung. Anhand der graphischen Darstellungen (siehe Abbildung \ref{fig:schatzeva} und \ref{fig:schatzeva2}) kann festgestellt werden, dass an manchen Stellen die Schätzung falsch lag und danach wieder richtig bewertet wurde. Das Problem tritt nur in vereinzelte Frames auf, wobei immer ein Fenster von mehreren Frames betrachtet wird und der Fehler damit nicht weiter untersucht werden muss. In Abbildung \ref{fig:schatzeva2} von Bild 700 bis 850 gibt es zwei Stellen, an denen die Körperhaltung vom Zustand \glqq{}Stehend\grqq{} auf \glqq{}Sitzend\grqq{} oder \glqq{}Beugend\grqq{} falsch bewertet wurden. Weil der Fokus in dieser Arbeit nicht speziell auf Schätzung der Körperhaltung sondern auf die Erkennung der außergewöhnlichen Situationen ist, kann das Problem mit den minimalen falschen Erkennungen im Graph vernachlässigt werden.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{fig/schatzungevaluation.pdf}
	\caption{Überprüfung der Schätzung der Körperhaltung am Testvideo \glqq{}fallen2.mp4\grqq{}. Die x-Achse stellt die Framenummer dar und die y-Achse ziegt die Indexe der Körperhaltung. Die blaue Gerade ist die Referenz-Körperhaltung und die gelbe Gerade ist die geschätzte Körperhaltung. Die Schätzung der Körperhaltung hat in diesem Beispiel eine Genauigkeit von 76,47\%.} 
	\label{fig:schatzeva}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{fig/schatzungevaluation2.pdf}
	\caption{Überprüfung der Schätzung der Körperhaltung an \glqq{}fallen2.mp4\grqq{}. Die Schätzung der Körperhaltung hat in diesem Beispiel eine Genauigkeit von 71,93\%.} 
	\label{fig:schatzeva2}
\end{figure}

\section{Erkennung außergewöhnlicher Situationen}
Im Abschnitt \ref{chp:BackgroundSubtraction} und \ref{chp:schatzung} wird eine Silhouette durch Bewegungen erstellt und eine Schätzung der Körperhaltung durch die Histogrammanalyse berechnet. Zur Erkennung außergewöhnlicher Situationen wird noch die aktuelle Position der Person in einem bestimmten Zeitraum berücksichtigt. An dieser Stelle ist es wichtig, dass die Anwendung den Unterschied zwischen normalen Bewegungen im Alltag und außergewöhnlichen Situationen erkennt. Wenn beispielsweise die Körperhaltung einer Person als \glqq{}Liegend\grqq{} geschätzt wird, kann es sein, dass die Person auf einem Sofa oder in einem Bett schläft. Wenn die Person im Bett liegt, handelt es sich um eine normale Situation. Deswegen ist es wichtig die Positionen der Möbeln im Raum zu identifizieren. Zur Erkennung dieser Positionen sind vier Ansätze in Betracht gezogen worden. Der erste Ansatz erfordert ein vorhandenes 3D-Modell des Raums, welches durch eine Kamera konstruiert werden kann. Diese Idee hat den Vorteil, dass die Positionen von Möbeln im Raum direkt identifiziert werden können und damit schnell festgestellt wird, ob sich eine Person auf dem Boden oder im Bett liegt. Die Arbeit für die Erstellung des 3D-Modells ist sehr zeitaufwendig und es ist nötig, für jeden neuen Raum ein neues 3D-Modell zu berechnen. Es gibt noch eine andere Techniken zur Identifizierung der Positionen von Möbel im Raum. Der zweite Ansatz betrachtet die \glqq{}Bird Eye View\grqq{}(BEV) Methode. Diese Technik wird bei Automobilen mit Rückfahrkamera angewendet. Der Nachteil ist: BEV braucht exakt eine exakt kalibrierte Kamera und alle Parameter des Kameramodells müssen bekannt sein, damit eine Transformation gemacht werden kann. Außerdem ist ein 2D-Modell der Wohnung benötigt, damit die Möbel in BEV erkannt werden können. Aus diesem Grund ist diese Methode für dieses Projekt nicht anwendbar, weil die Kamera an einer beliebigen Position im Raum aufgestellt werden kann. Der dritte Ansatz ist die neuronale Netze. Diese Methode ist häufig im Bereich von Objekterkennungen angewendet. Um die Objekte richtig zu erkennen, brauchen die neuronale Netze riesige Daten von Objekte. Die neuronale Netze werden anhand der Größe der Datenmenge bei dem Trainieren verbessert. In diesem Projekt wird ein vor-trainiertes Netz zur Erkennung der Möbeln angewendet. Das vor-trainiertes Netz kann ein Sofa mit einer Wahrscheinlichkeit von 28,80\% und einen Stuhl mit einer Wahrscheinlichkeit von 25,76\% erkennen (siehe Abbildung \ref{fig:nn}). Außerdem ist der Boden auch als ein Sofa, die Testperson als eine Katze falsch klassifiziert. Dies neuronale Netz kann mit mehrere Bilder weiter trainiert, um ein bessere Ergebnis zu liefern. Das Trainieren eines neuronalen Netzes ist aufwendig und benötigt eine große Menge von Bildern. Die sind zwei Beschränkungen dieses Ansatzes. 

\begin{figure}[H]
	\centering
	\includegraphics[width=0.95\textwidth]{fig/neurolnet.png}
	\caption{Objekterkennung mit einem neuronalen Netz.}
	\label{fig:nn}
\end{figure}

Um das Problem der Unterscheidung von normalen und abnormalen Situationen zu lösen, wird eine logische Methode, die als Fuzzylogik genannt wird, verwendet. Dies ist der vierte Ansatz, der auch in diesem Meilenstein befolgt wurde. Die Positionen der Möbeln im Raum sind auf dem Bild von Benutzern zunächst vordefiniert. Die Fuzzylogik berechnet durch die Positionen der Möbel (zum Beispiel: Boden), die aktuelle Körperhaltung und die aktuelle Zeit. Dann wird die Wahrscheinlichkeit ausgerechnet, ob es sich um eine außergewöhnliche Situation handelt. Wie in Abschnitt \ref{sec:fuzzylogik} schon erwähnt, besteht die Fuzzylogik aus drei Schritten: Umwandlung von Eingaben in Fuzzymengen, Anwendung von vordefinierten (IF-ELSE) Regeln und Defuzzifizierung (siehe Abbildung \ref{fig:fuzzylogik}).

\begin{figure}[H]
	\centering
	\includegraphics[width=0.85\textwidth]{fig/fuzzylogik.pdf}
	\caption{Dritter Meilenstein}
	\label{fig:fuzzylogik}
\end{figure}

Zuerst müssen die Mitgliederfunktionen der Eingaben definiert werden und die sind wie folgt beschrieben:

\begin{itemize}
	\item Die Körperhaltung besteht aus vier Zuständen: Stehend (Index: 0), Liegend (Index: 1), Beugend (Index: 2), Sitzend (Index: 3). Diese sind als Punkte im Fuzzymodel definiert (siehe Abbildung \ref{fig:fuzzyfunktion}a).
	\item Die Zeit besteht aus Tag und Nacht und ist mit glockenförmige Funktionen mit Parametern $a=6$, $b=10$ mit $mean=14$ für Tag und $mean=2$ für Nacht definiert (siehe Abbildung \ref{fig:fuzzyfunktion}b). Der Wertbereich für die Zeit ist ein Intervall $[0, 24]$ für $24$ Stunden am Tag. Diese Funktion kann wie folgt beschrieben werden\cite{rosenfeld1971fuzzy}:
	\begin{equation}
	f(a, b, mean) = \frac{1}{1 + |\frac{x - mean}{a}|^{2b}}
	\end{equation} 
	\item Die normalen Koordinaten im Raum sind die Positionen, an denen die Testperson normalerweise liegt, sitzt oder steht. Diese wird mit Gaußschen-Funktionen definiert, wobei die Mittelwerte der Funktion $xy$-Koordinaten sind und die Standardabweichung $\sigma = 30$ Pixel beträgt (siehe Abbildungen \ref{fig:fuzzyfunktion}c und \ref{fig:fuzzyfunktion}d). Dies entspricht der Größe eines 1,5m lang Sofas mit einem Abstand 3m von der Kamera. Der Wertbereich für die x-Koordinate ist ein Intervall [0, Breite des Bildes] und für y-Koordinate ist ein Intervall [0, Höhe des Bildes].
	\item Die Ausgabe ist der \glqq{}Status\grqq{}. Dieser besteht aus den Komponenten \glqq{}good\grqq{} und \glqq{}bad\grqq{}, welche als Fuzzymenge zusammengeführt werden. Die Komponente \glqq{}good\grqq{} steht dabei für die Wahrscheinlichkeit, dass es sich um eine normale Situation und \glqq{}bad\grqq{} gibt die Wahrscheinlichkeit an, dass es sich um eine außergewöhnliche Situation handelt. Der \glqq{}Status\grqq{} besitzt damit zwei Gaußsche-Funktionen mit je Standardabweichung $\sigma = 1$ mit Mittelwert $\mu=4$ für abnormale und $\mu=6$ für normale Situationen (siehe Abbildungen \ref{fig:fuzzy1}, \ref{fig:fuzzy2}).
\end{itemize}  

Die oben vordefinierten Mitgliederfunktionen werden in der Abbildung \ref{fig:fuzzyfunktion} graphisch dargestellt. Zur Anwendung von (IF-ELSE) Regeln (siehe Algorithmus \ref{algo:fuzzy_modell}) wird das Minimum als Aktivations-Methode und das Maximum als Akkumulations-Methode eingesetzt. Die Regeln im Algorithmus \ref{algo:fuzzy_modell} können wie folgt interpretiert werden:

\begin{itemize}
	\item Es ist \glqq{}bad\grqq{}, wenn die Person nicht auf dem Sofa liegt.
	\item Es ist \glqq{}good\grqq{}, wenn die Person auf dem Sofa liegt.
	\item Es ist \glqq{}bad\grqq{}, wenn die Person am Tag liegt.
	\item Es ist \glqq{}good\grqq{}, wenn die Person am Tag steht.
	\item Es \glqq{}bad\grqq{}, wenn die Person am Tag sitzt oder sich beugt.
\end{itemize}

Bei Defuzzifizierung, die eine Abbildung der Ausgabe in eine scharfe Mengen durchführt, wird die Schwerpunktmethode verwendet. Die oben vordefinierten Parameter können anhand des Raums und biologischer Uhr der Testperson angepasst werden. Zum Beispiel gibt es in einem Raum nicht nur ein Sofa sondern auch ein Bett, dann müssen die Koordinaten des Bettes noch definiert werden. Das hilft dem Programm zu erkennen, ob eine Person im Bett und nicht auf dem Boden ist. Die Anpassung der Parameter kann durch maschinelles Lernen nach dem Gebrauch des Benutzers verbessert werden.\\

Die Fuzzylogik berechnet, wie hoch die Wahrscheinlichkeit ist, dass sich die aktuelle Situation um eine normale Situation handelt. Für komplexe Anwendungsfälle wurden Gaußsche-Funktionen benutzt, um eine Demo-Software zu realisieren. Es wurde angenommen, dass die Koordinaten von einem Sofa im Raum $(591, 441)$, die aktuelle Zeit $0$ Uhr nachts und $8$ Uhr morgens sind und dass die Testperson an der Position (574, 424) liegt (Wie in die Fuzzymenge in Abbildung \ref{fig:fuzzyfunktion} eingetragen). In dem ersten Testfall, indem die Testperson nachts auf dem Sofa lag, ergab sich die Wahrscheinlichkeit $good$=93,29\%, dass es sich um eine normale Situation handelt. Für eine außergewöhnliche Situation wurde eine Wahrscheinlichkeit von $bad$=26,59\% berechnet (Abbildung \ref{fig:fuzzy1}). In Regeln \ref{algo:fuzzy_modell} gibt es einen gemischten Fall, wobei die Testperson auf dem Sofa um $8$ Uhr Morgen liegt. Hier ist es gut, wenn die Person auf dem Sofa und nicht auf dem Boden liegt und schlecht, dass die Person morgens auf dem Sofa liegt. Dafür ergab die Rechnung eine Wahrscheinlichkeit von $good$=71,6\% für eine normale Situation und $bad$=49,71\% für eine abnormale Situation (Abbildung \ref{fig:fuzzy2}). Eine Situation wird genau dann als schlechte Situation klassifiziert, wenn gilt: $bad \geq 80\%$ und $bad > good$. Diese Bedingungen können wie folgt definiert werden:
\begin{eqnarray}
\texttt{Situation schlecht} &\Leftrightarrow& bad \geq 80\% \wedge bad > good \\
\texttt{Situation gut} &\Leftrightarrow& bad < 80\% \vee bad <= good
\end{eqnarray}

\begin{algorithm}[H]
	\caption{Regeln für die Auswertung einer Situation}
	\label{algo:fuzzy_modell}
	\If{(posture IS laying) AND (NOT xposition IS good) OR (NOT yposition IS good)}{status IS bad;}
	\If{(posture IS laying) AND (xposition IS good) AND (yposition IS good)}{status IS good;}
	\If{(posture IS laying) AND (time IS day)}{status IS bad;}
	\If{(posture IS standing) AND (time IS day)}{status IS good;}
	\If{((IF posture IS sitting) OR (posture IS bending)) AND (time IS night)}{status IS bad;}
\end{algorithm}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.75\textwidth]{fig/fuzzyfunktion.pdf}
	\caption{Die Mitgliederfunktionen der Körperhaltung, Zeit und aktuelle Position der Person. Der Eingabebereich der Zeit ist von 0 bis 24 Uhr beschränkt und die Eingabe der Position muss ein positiver Wert sein. Diese Abbildung stellt ein Beispiel dar, wobei eine Person um 8 Uhr auf dem Sofa (x=574, y=424) liegt.} 
	\label{fig:fuzzyfunktion}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.59\textwidth]{fig/fuzzy1.pdf}
	\caption{Testfall: Person liegt auf dem Sofa um 12 Uhr Nachts. Das Programm liefert ein Ergebnis von 93,29\% für normale und 26,59\% für abnormale Situation.} 
	\label{fig:fuzzy1}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.59\textwidth]{fig/fuzzy2.pdf}
	\caption{Testfall: Person liegt auf dem Sofa um 8 Uhr Morgens. Das Programm liefert ein Ergebnis von 71,6\% für normale und 49,71\% für abnormale Situation.} 
	\label{fig:fuzzy2}
\end{figure}

Bis jetzt können die außergewöhnlichen Situationen von normalen Bewegungen im Alltag mit Hilfe von Fuzzylogik unterschieden werden. Die außergewöhnlichen Situationen sollen nicht nur in einem Bild sondern in einer Sequenz von mehreren Bildern beachtet werden, damit das System bessere Entscheidungen für eine abnormale Situation treffen kann. Wenn immer nur ein Bild beachtet wird, könnte eine falsch klassifizierte Situation in nur einem Bild bereits einen Alarm auslösen. Um die Beobachtung in mehreren Bildern zu realisieren, wird ein Modell gebaut, wobei die erkannte Person in einer Begrenzungsbox in einem Zeitraum betrachtet wird. Das heißt, jedes \inlinecode{C++}{Personen}-Objekt besteht aus einer aktuellen Begrenzungsbox $B_{current}$, die nach der Hintergrundsubtraktion erstellt wird. Nach jedem eingegangenen Bild wird die neue Begrenzungsbox $B_{new}$ mit der aktuellen Begrenzungsbox $B_{current}$ verglichen, um zu bestimmen, ob es sich um die selbe Person handelt und ob die Person sich bewegt hat. Eine globale Variable \inlinecode{C++}{movementMaximum} gibt die maximale zulässige Geschwindigkeit an, mit der sich eine Person bewegen kann. Der euklidische  Abstand zwischen $B_{current}$ und $B_{new}$ wird berechnet und mit der \inlinecode{C++}{movementMaximum} verglichen. Wenn dieser euklidische Abstand größer als \inlinecode{C++}{movementMaximum} ist, wird eine neue Person erstellt und die aktuelle Person wird ignoriert. Wenn dieser Abstand kleiner oder gleich  \inlinecode{C++}{movementMaximum} ist, wird $B_{current}$ der Person mit $B_{new}$ ersetzt (siehe Algorithmus \ref{algo:gleicheperson}).

\begin{algorithm}[H]
	\caption{Aktualisierung der Begrenzungsbox}
	\label{algo:gleicheperson}	
	\begin{algorithmic}
		\IF {$dist(B_{current}, B_{new}) \leq$  \inlinecode{C++}{movementMaximum}}
		\STATE $B_{current} \gets B_{new}$
		\ELSE 
		\STATE Create new bounding box $B_{new}$ for new person
		\ENDIF
	\end{algorithmic}
\end{algorithm}

Mit der weiteren Variable \inlinecode{C++}{movementMinimum} wird überprüft, ob sich  die Person bewegt. Diese Variable hilft bei der Erkennung von außergewöhnlichen Situationen, in denen die Person zum Beispiel liegt und sich nicht mehr bewegt. Wenn der Abstand zwischen der aktuellen Begrenzungsbox $B_{current}$ und  der neuen Begrenzungsbox $B_{new}$ kleiner als \inlinecode{C++}{movementMinimum} ist, bedeutet es, dass die Person sich nicht mehr bewegt. Dazu werden Breite und Höhe von $B_{current}$ und $B_{new}$ verglichen, um zu bestimmen, ob es eine Änderung der Begrenzungsbox gibt (siehe \ref{algo:gleicheposition}).\\

\begin{algorithm}[H]
	\caption{Bedingung zur Erkennung von Bewegung einer Person}
	\label{algo:gleicheposition}	
	\begin{algorithmic}
		\IF {$dist(B_{current}, B_{new}) \geq$ \inlinecode{C++}{movementMinimum}\\
			$\OR$ $|width(B_{current})  - width(B_{new})|$ $\geq$ \inlinecode{C++}{movementMinimum}\\
			$\OR$ $|heigth(B_{current})  - heigth(B_{new})|$ $\geq$ \inlinecode{C++}{movementMinimum}
		}
		\STATE Person IS moving
		\ELSE 
		\STATE Person IS NOT moving
		\ENDIF
	\end{algorithmic}
\end{algorithm}

Bis jetzt kann das System eine außergewöhnliche Situation mit folgenden Bedingungen erkennen:\\
\begin{itemize}
	\item Durch Hintergrundsubtraktion und Schätzung der Körperhaltung wird eine Person erkannt, ob sie steht, liegt, sich beugt oder sitzt.
	\item Mit Hilfe von Fuzzylogik kann das System bewerten, ob eine Person in einem normalen oder nicht normalen Ort (z.B. Boden) liegt.
	\item Das System kann einer Person verfolgen und erkennen, ob sie sich bewegt.
\end{itemize}
Ziel des gesamten Systems ist die Erkennung von außergewöhnlichen Situationen. Dabei wird wie folgt vorgegangen: Es wird mit Hilfe von Hintergrundsubtraktion eine Silhouette aus jedem Frame der Kamera und die dazugehörige Begrenzungsbox berechnet. Danach wird eine Histogrammanalyse auf die erzeugte Silhouette vorgenommen. Diese Analyse liefert bereits eine geschätzte Körperhaltung für die Person auf dem Bild. Anschließend entscheidet die Fuzzylogik, ob es sich ob eine außergewöhnliche Situation handelt. Dabei betrachtet die Fuzzylogik die aktuelle Zeit, Ort der Person im Raum und seine berechnete Körperhaltung. Weiterhin werden mehrere Frames beobachtet. Falls 30 aufeinander folgende Frames einer außergewöhnlichen Situation zugeordnet werden, wobei sich die beobachtete Person nicht ändert, handelt es sich um einen Notfall. Dazu wird für jede erkannte Person ein Zähler \glqq{}badcounter\grqq{} zugeordnet. Dieser Zähler speichert die Anzahl auf wie viele aufeinander folgende Frames sich die jeweilige Person in einer außergewöhnliche Situation befand. Der folgende Algorithmus \ref{algo:finalAlgo} beschreibt das genannte Verfahren.

\begin{algorithm}[H]
	\caption{Erkennung von außergewöhnlichen Situationen}
	\label{algo:finalAlgo}
	\KwData{b : input image}
	\KwResult{Situation: normal or not normal}
	\Parameter{$bad\_frames\_limit \gets 30$}
	\begin{algorithmic}
		
		\STATE $s:Silhouette$, $B_{new}:Boundingbox$ $\gets$ Backgroundsubtraction($b$)
		\STATE $k:Posture$ $\gets$ Histogrammanalysis($s$)
		\STATE $t:Time$ $\gets$ current time
		\STATE $z:State$ $\gets$ Fuzzylogic($B_{new}$, $ k$, $t$)
		\\
		\COMMENT{State hat die zwei Komponenten: \glqq{}good\grqq{} und \glqq{}bad\grqq{}. Jede mit einer berechneten Wahrscheinlichkeit und sagt, wie die Situation von Fuzzymodell bewertet wird.}
		
		\IF {same person is detected}
		\STATE $p:Person$ $\gets$ detected person
		\STATE Update boundingbox $B_{current}$ of $p$
		
		\IF{$p$ IS NOT moving $\AND$ $k$ = LAYING $\AND$ $z$ = BAD}
		\STATE $p.badcounter$ ++
		\ELSE
		\STATE $p.badcounter$ $\gets$ 0
		\ENDIF
		\ELSE
		\STATE $p:Person$ $\gets$ new Person
		\STATE $B_{current}$ of $p$ $\gets$  $B_{new}$
		\ENDIF
		
		\IF{$p.badcounter$ $\geq$ $bad\_frames\_limit$}
		\STATE Trigger alarm
		\ENDIF
		
	\end{algorithmic}
\end{algorithm}	

\section{Anpassung des Verfahrens auf nicht-statische Kameras}
Die Innenkamera von Bosch kann sich 360° um die eigene Achse drehen. Durch den eingebauten Bewegungsmelder kann sie autonom ein sich bewegendes Objekt verfolgen und fokussieren. Dies hilft bei der Erkennung außergewöhnlicher Situationen, da sich die Kamera auf die sich bewegenden Personen fokussieren kann. Außerdem besitzt diese Kamera einen großen Vorteil gegenüber anderen Kameras, da sie an einem beliebigen Ort im Raum aufgestellt werden kann und somit immer den gesamten Raum abdecken und beobachten kann. Wenn die Kamera eine Drehung vornimmt, um einer Person zu folgen, werden alle Pixel des aufgenommenen Bilds geändert. Das führt dazu, dass die Hintergrundsubtraktion mit dem statischem Hintergrund nicht mehr funktioniert. Um das Problem mit der Hintergrundsubtraktion bei Drehung der Kamera zu lösen, wird für jedes erhaltene Binärbild eine Hintergrunddichte mit Hilfe des OpenCV Frameworks berechnet. Die \inlinecode{C++}{backgroundDensity} repräsentiert das Verhältnis von der Anzahl an schwarzen Pixel (Hintergrund) zu der gesamten Pixelanzahl. Wenn sich die Kamera dreht, werden viele oder sogar alle Pixel verändert und dies führt dazu, dass der Vordergrund (weiße Pixel) fast das gesamte Binärbild ausmacht. Der Anteil von dem Hintergrund wird dabei sehr gering sein und es kann durch einen Schwellenwert für die \inlinecode{C++}{backgroundDensity} erkannt werden, ob sich die Kamera dreht.
Nach der Erstellung des Binärbilds, wobei sich die Kamera nicht bewegt hat, kann nun die Silhouette näher betrachtet werden. Die Silhouette wird auf zusammenhängende Regionen durchsucht. Dabei sind zwei Pixel $p_{i}$ und $p_{j}$ genau dann zusammenhängend, wenn $p_{i}$ ein Nachbar von $p_{j}$ ist. Jedes Pixel hat dabei genau acht Nachbaren, außer die Pixel, die am Rand des Bilds liegen. Die so erhaltenen zusammenhängenden Regionen werden von $1$ bis $n$ nummeriert. Der schwarze Hintergrund steht dabei für die Region mit der Nummer $0$.\\
Mit der Funktion \inlinecode{C++}{connectedComponentsWithStats()} in OpenCV werden die zusammenhängenden Regionen und deren Fläche, abhängig von der Pixelanzahl, berechnet. Wenn die Hintergrunddichte \inlinecode{C++}{backgroundDensity} kleiner als der Schwellenwert $80\%$ ist, handelt es sich um eine stärke Änderung des Hintergrundes. Die selbst aufgenommenen Testvideos wurden mit dem Programm getestet. Dabei wurde eine Durchschnittsdichte des Hintergrunds von über $80\%$ gemessen und wurde anschließend als Schwellenwert verwendet. Wenn die Bedingung  \inlinecode{C++}{backgroundDensity} $\leq$ 0,8 erfüllt ist, dann wird das Hintergrundbild mit dem aktuellen Bild ersetzt und alle registrieren sich bewegenden Personen gelöscht.\\
Wenn nur die Bedingung \inlinecode{C++}{backgroundDensity} $\leq$ 0,8 betrachtet wird, kann es sein, dass das Bild als Hintergrund genommen wird, welches kurz vor Stillstand der Kamera zustande kommt. Das führt zu einem falschen Modell und zu schlechten Ergebnissen des Programms. Um nach der Bewegung der Kamera das erste Bild als Hintergrund zu markieren, wird ein Auslöser \inlinecode{C++}{trigger} verwendet und sorgt dafür, dass das richtige Bild (nach Stillstand der Kamera) als Hintergrund markiert wird. Wenn die Bedingung \inlinecode{C++}{backgroundDensity} $\leq$ 0,8 erfüllt ist, wird der Auslöser aktiviert und nach entweder einer Sekunde oder $30$ Bilder wird das aktuelle Bild als Hintergrund markiert.\\
In Abbildungen \ref{fig:false1} und \ref{fig:false2} zeigen links und rechts Screenshots. Die linken zeigen die Markierung des Hintergrunds ohne Auslöser und rechts mit Auslöser. Der Algorithmus \ref{algo:newBG} macht die Hintergrundersetzung für die 360° Kamera deutlich.

\begin{algorithm}[H]
	\caption{Ersetzung des Hintergrundbilds bei Drehung der Kamera}
	\label{algo:newBG}	
	\KwData{$binaryImage \gets$ Binary image from background subtraction}
	\KwResult{Replacing background image while camera is rotating}
	\begin{algorithmic}
		\STATE $region[~] \gets$ \inlinecode{C++}{connectedComponentsWithStats}$(binaryImage)$ 
		\STATE  \inlinecode{C++}{backgroundDensity} $\gets region[0] / imageSize$
		\STATE  \inlinecode{C++}{trigger} $\gets$ FALSE
		
		\IF  {\inlinecode{C++}{backgroundDensity} $ \leq 0.8$}
		\STATE \inlinecode{C++}{trigger} $\gets$ TRUE
		\STATE $counter \gets 0$
		\ENDIF
		
		\IF  {\inlinecode{C++}{trigger}}
		\STATE $counter$++
		\IF  {$counter \geq 30$ }
		\STATE Replace background image with current image 
		\STATE Delete registered moving persons
		\ENDIF
		\ENDIF
		
	\end{algorithmic}
\end{algorithm}	


\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{fig/false1.pdf}
	\caption{Linke Seite: falsche Erkennung der Bewegung passiert, wenn das Hintergrundbild vor dem Stillstand der Kamera ersetzt wird. Rechte Seite: richtige Erkennung nach Drehung der Kamera.} 
	\label{fig:false1}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{fig/false2.pdf}
	\caption{Linke Seite: eine weitere falsche Erkennung der Bewegung. Rechte Seite: ein richtige Erkennung der Bewegung.}
	\label{fig:false2}
\end{figure}
