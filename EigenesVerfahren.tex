\chapter{Eigenes Verfahren}\label{chp:EigVerfahren}
Diese Arbeit besteht aus drei große Meilensteine, nämlich Hintergrundsubtraktion, Schätzung der Körperhalterung mit Histogrammanalyse und Erkennung außergewöhnlicher Situation mit Fuzzylogik. Um mein eigenes Verfahren einfacher zu verstehen, stellt Die Abbildung \ref{fig:allgemein} den allgemeinen Prozess der drei Meilensteine dar. Die in drei verschiedenen Farben gezeigt werden.\\
Die alle Testvideos haben eine Auflösung von 1280x720 Pixel und die werden mit 15 Frames pro Sekunde aufgenommen. Die Hintergrundsubtraktion wird mit Blau dargestellt. Die Eingabe für Hintergrundsubtraktion ist ein von der Kamera aufgenommenes Bild und die Ausgabe ist eine Silhouette einer Person. Der nächste Meilenstein ist die Histogrammanalyse, die mit Gelb gezeigt wird. Die Eingabe dieses Meilensteins ist die Silhouette des letztes Schritts und die Ausgabe ist eine Schätzung der Körperhaltung der Person. Der letzte Meilenstein ist die Fuzzylogik, die mit der roten Farbe vorgestellt wird. Die Eingaben der Fuzzylogik sind die Schätzung der Körperhaltung, die aktuelle Position der Person ud die aktuelle Zeit. Die Aufgabe der Fuzzylogik ist eine Entscheidung, ob es um eine außergewöhnliche Situation sich handelt.

\begin{figure}[H]			
	\centering
	\includegraphics[width=0.7\textwidth]{fig/allgemein.pdf}
	\caption{Drei Meilensteine zur Erkennung von außergewöhnlichen Situation. Der Erster Meilenstein ist Hintergrundsubtraktion, die eine Silhouette einer Person ausgibt. Der nächste Meilenstein ist die Histogrammanalyse, deren Aufgabe ist die Schätzung eine Körperhaltung. Am Ende ist die Fuzzylogik, die eine Entscheidung ausgibt, ob eine außergewöhnliche Situation passiert.}
	\label{fig:allgemein}
\end{figure}

\section{Hintergrundsubtraktion}\label{chp:BackgroundSubtraction}
\subsection{Vergleich verschiedener Methoden}

In dieser Masterarbeit ist Hintergrundsubtraktion ein wichtiger Baustein. Die Abbildung \ref{fig:bgsub} zeigt allgemeine Grundfunktion von der Hintergrundsubtraktion. Ein großes Problem ist wie ein korrekter Hintergrund erstellt werden kann, damit ein sich bewegtes Objekt robust erkannt werden kann. 

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{fig/bgsub.pdf}
	\caption{Erster Meilenstein geht um das Hintergrundsubtraktion-Verfahren. Die Eingabe ist ein aktuelles Bild und die Ausgabe ist ein Binärbild mit der Silhouette. Bei Aktualisierung des Hintergrundmodells werden nur selektive Bereiche aktualisiert.}
	\label{fig:bgsub}
\end{figure}
Für diese Arbeit wurden populäre Methoden aus Kapitel \ref{sec:grunglagen_hintergrundsub} (Gaussian Mixture Model, K-nächste Nachbar, Kern Density Estimation und Vibe) ausprobiert und verglichen. Alle genannten Methoden wurden schon in \ref{sec:grunglagen_hintergrundsub} theoretisch beschrieben. In diesem Abschnitt geht es um die Bewertungen der Hintergrundsubtraktionsverfahren für den gegebenen Anwendungsfall. Bei \acs{AGMM} wird eine Mischung von 5 Gaußschen Verteilungen für deas Hintergrundmodell verwendet und 100 letzten Bildern werden als \grqq{}History\grqq{} des Hintergrundmodells angewendet. Mit einer Lernrate von $0.01$ funktioniert das \acs{AGMM} Verfahren gut in dem Test. Für das \acs{KNN} Verfahren wurden auch die gleichen Parameter genommen, um den Vergleich der Hintergrundsubtraktionsverfahren objektiv zu bewerten. Bei \acs{KDE} handelt es sich um eine Berechnung von Intensitätswerte für einen Pixel, deshalb wird nur ein Parameter von 100 als \grqq{}History\grqq{} (wie bei \acs{AGMM}) eingesetzt. Wie in \cite{barnich2009vibe} beschrieben, ist Vibe ein nicht-parametrisches Verfahren, deshalb musste kein Parameter konfiguriert werden.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{fig/motiondetrection.png}
	\caption{Vergleichen von verschiedene Techniken zum Hintergrundsubtraktion anhand eines Repräsentationsbeispiels (a) Weichzeichnen (b) AGMM (c) KNN (d) KDE (e) Vibe} 
	\label{fig:compare_bgsubtraction}
\end{figure}
Aus dem Beispiel in \ref{fig:compare_bgsubtraction} wird deutlich, dass  \acs{KNN} und  \acs{Vibe} bessere Ergebnisse als \acs{AGMM} und \acs{KDE}  liefern. Es gibt weniges Rauschen in Ergebnisbildern von \acs{KNN} und  \acs{Vibe}. Hier ist kritisch anzumerken, dass  \acs{Vibe} und  \acs{KDE} längere Verarbeitungszeit als den Rest brauchen. Das erste Ziel der Arbeit ist die Untersuchung der Hintergrundsubtraktion, damit eine optimale Silhouette erkannt werden kann. Anhand des Schaubildes ist das \acs{KNN}-Verfahren ein guter Kandidat für Hintergrundsubtraktion ist. Viele Videos in verschiedenen Räumen mit verschiedene Körperhaltung wurden für diese Arbeit aufgenommen, damit die Qualität der Software gut geprüft wird.  Anhand der Abbildung \ref{fig:compare_bgsubtraction} und Verarbeitungszeit wird deutlich, dass  \acs{KNN} vernünftige Methode für unser Ziel und deswegen wird  \acs{KNN} für diese Arbeit entschieden.
Nach sorgfältiger qualitativer Begutachtung der Ergebnisse der Verfahren KNN, Vibe, AGMM, KDE... auf dem Videomaterial habe ich mich für dir Verwendung von KNN entscheiden, weil KNN und Vibe bessere Silhouetten liefern können und KNN kürzere Verarbeitungszeit als Vibe hat. Beispielweise zeigt die Abbildung \ref{fig:vibeknn} die Verarbeitungszeit von KNN und Vibe an einem Testvideo von 1116 Bildern.\\

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{fig/knnvibe.pdf}
	\caption{Die Verarbeitungszeit von KNN und Vibe werden verglichen. Es ist deutlich, dass KNN eine kürzere durchschnittliche Verarbeitungszeit (107ms) als Vibe (551ms) hat.} 
	\label{fig:vibeknn}
\end{figure}

Die oben genannten Hintergrundsubtraktionsverfahren basieren auf der Änderung der Intensitätswerten jedes Pixel, deshalb liefern die Verfahren ein Binärbild mit viel Rauschen zurück. Um das Rauschen zu entfernen, werden Erosion und Dilatation zum Einsatz gebracht, die zwei Verfahren sind zwei grundlegende Operationen bei der morphologischen Bildverarbeitung, auf der alle anderen morphologischen Operationen basieren. Die Kombination von der zweit Methoden ist \glqq{}Öffnung\grqq{} (eng. Opening) Methode genannt\cite{zamperoni2013methoden}.\\

Erosion ist ein Verfahren, dass ein Bild mit einem einfachen, vordefinierten Filter untersucht und daraus Schlussfolgerungen zieht, wie dieser Filter in die Formen im Bild passt. Wenn ein Pixel und seine Nachbarpixel vollständig von dem Filter überlagert werden, wird das Pixel behaltet, sonst wird das Pixel gelöscht. Angenommen ist $A$ ein Binärbild und $B$ ein Filter mit einem Zentrum in der Mittel von $B$. Wenn $B$ sich in $A$ bewegt, überlagert für jedes Pixel in $A$ der Ursprung von $B$. Wenn $B$ vollständig in $A$ enthalten ist, wird das Pixel bei Erosion beibehalten, ansonsten gelöscht\cite{zamperoni2013methoden}.\\

Dilatation ist ein gegenteiliges Verfahren zur Erosion, es versucht die Form eines Bildes zu vergrößern. Wenn $B$ sich in $A$ bewegt, überlagert für jedes Pixel in $A$ der Ursprung von $B$ wie Erosion. Die alle Pixel in $A$, die vollständig von $B$ überlagert sind, werden beibehalten oder markiert\cite{zamperoni2013methoden}.\\

Um die zwei Operationen besser zu verstehen, wurden zwei Beispiele für Erosion und Dilatation in Abbildungen \ref{fig:erosion} und \ref{fig:dilatation} dargestellt.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{fig/erosion.pdf}
	\caption{Ein Beispiel für Erosion mit einem 3x3 Filter. Die Form des Bildes wird nach Erosion verkleinert.} 
	\label{fig:erosion}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{fig/dilatation.pdf}
	\caption{Ein Beispiel für Dilatation mit einem 3x3 Filter. Die Form des Bildes wird nach Dilatation vergrößert.} 
	\label{fig:dilatation}
\end{figure}

Erosion wird in diesem Fall angewendet, um das Rauschen nach der Hintergrundsubtraktion zu entfernen. Es folgt eine Verkleinerung der Silhouette. Um die Verkleinerung zu vermeiden wird Dilatation hier genutzt. Nach der \grqq{}Öffnung\grq{} Methode wird eine Silhouette ohne Rauschen in einem Binärbild erstellt. Die Abbildung \ref{fig:eroanddila} zeigt, dass nach Erosion die Silhouette kleiner wird. Um dies zu kompensieren, wird eine Dilatation verwendet.

Die Abbildung \ref{fig:eroanddila} zeigt, dass nach Erosion unerwartetes Rauschen außer Silhouette entfernt ist und auch die Silhouette leicht geschädigt ist. 


\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\textwidth]{fig/eroanddila.pdf}
	\caption{Ein Beispiel für Erosion und Dilatation. Nach der Erosion das unerwartetes Rauschen außer der Silhouette entfernt ist. Außerdem wird die Silhouette verkleinert. Nach der Dilatation wird die Silhouette wie die originale Form vervollständigt.} 
	\label{fig:eroanddila}
\end{figure}

\subsection{Verbesserungen für Hintergrundsubtraktion}
Im Allgemein wird für jedes Pixel vom aktuellen Bild zuerst berechnet, ob er zu Hintergrund oder Vordergrund gehört. Dann wird den Pixel wieder nach jedem Bild in Hintergrundmodell berücksichtigt. Das bedeutet, wenn ein Mensch sich von der Position $P_1$ nach $P_2$ bewegt und auf $P_2$ lange zum Stillstand kommt, wird der Mensch wieder den Hintergrund zugeordnet. Dies Problem muss unbedingt gelöst werden, weil die Arbeit zur Erkennung einer außergewöhnlichen Situation auch die Information einer Person, die sich nicht bewegt, benötigt, um einer eventuellen Unfall zu detektieren. Zum Beispiel passiert ein Unfall und die Person liegt lange auf dem Boden.
 
\subsubsection{Erste Verbesserung (Aktualisierung der selektiven Begrenzungsboxen)}
Das oben genannte Problem liegt an der Aktualisierung des Hintergrundmodells, deshalb wird eine Methode an dieser Stelle erstellt, um das Problem zu vermeiden (Abbildungen \ref{fig:mymethod1}, \ref{fig:mymethod2}, \ref{fig:mymethod3}). Die Methode ist \glqq{}Aktualisierung der selektiven Begrenzungsboxen\grqq{} (ASB) genannt. Bei Aktualisierung eines Hintergrundbildes soll nur die Pixel, die nicht zu Vordergrund zugeordnet sind, betrachtet werden. Das erste Bild wird in Grauwertbild mit der Originalgröße (1280x720) und dem Wertbereich $[0,255]$ konvertiert und als ein Hintergrundbild $H$ gespeichert. Der Unterschied zwischen dem aktuellen Grauwertbild $I$ und dem abgespeicherten Hintergrundbild $H$ wird pixelweise mit absoluter Subtraktion gerechnet. Anschließend ergibt es sich ein Bild $S$, das den Unterschied zwischen dem aktuellen Bild und dem Hintergrundbild  mit den Werten von 0 bis 255 darstellt  (siehe Abbildungen \ref{fig:mymethod1}c, \ref{fig:mymethod2}c, \ref{fig:mymethod3}c). Je geringer der Wert von einem Pixel auf dem Bild $S$ ist, desto höher die Wahrscheinlichkeit, dass der Pixel zum Hintergrund gehört. Und umgekehrt je höher der Wert von einem Pixel auf dem Bild $S$ ist, desto höher die Wahrscheinlichkeit, dass das Pixel zu Hintergrund gehört. Nach der Berechnung des Unterschiedsbild wird ein Schwellwert von $50$ angewendet, um ein Binärbild zu erzeugen. Wenn ein Pixel ein Wert unter $50$ hat, wird das Pixel in Schwarz markiert und höher als $50$ dann wird das Pixel in Weiß markiert (Abbildungen \ref{fig:mymethod1}d, \ref{fig:mymethod2}d, \ref{fig:mymethod3}d).
\\
Bis jetzt ist ein Binärbild kreiert, das ein sich bewegende Objekt enthält.  Nun werden Erosion und Dilatation angewendet, um die Qualität des Binärbildes zu verbessern  (siehe Abbildungen \ref{fig:mymethod1}e, \ref{fig:mymethod2}e, \ref{fig:mymethod3}e). Wie im Abschnitt \ref{sec:grunglagen_hintergrundsub} schon erwähnt, besteht Hintergrundsubtraktion aus drei Hauptschritten: Initialisierung des Hintergrundes, Erkennung des Vordergrundes und Aktualisierung des Hintergrundes.\\

Die ersten zweit Schritte sind schon geschafft und es fehlt nur noch der letzte Schritte.  Ein Begrenzungsbox (eng. Bounding box) wird an dieser Stelle erstellt, um das sich bewegende Objekt zu begrenzen und auch für die Erkennung der Körperhalterung im nächsten Abschnitt zu unterstützen.\\

Zuerst wird Gruppen von Pixel, die kontinuierlich neben einander sind, markiert. OpenCV bietet eine Funktion an, die  \inlinecode{C++}{cv2.findContours()} heißt. Die Funktion \inlinecode{C++}{findContours} nimmt ein Binärbild als Eingabe und gibt Konturen von verbundenen Pixel als Ausgabe. Mit dieser Funktion wird die Silhouette mit einer Begrenzungsbox bestimmt. Mit Hilfe von der Funktion  \inlinecode{C++}{cv.contourArea()} kann die Begrenzungsbox eines Objektes einfach als Rechtecken herausgefunden werden (sie Abbildungen \ref{fig:mymethod1}f, \ref{fig:mymethod2}f, \ref{fig:mymethod3}f).\\

Nun ergibt es sich eine Begrenzungsbox $B_{aktuell}$, die sich bewegenden Objekte begrenzt. Wenn eine Person von Postion $P_1$ nach $P_2$ sich bewegt, wird eine Liste der vergangen Begrenzungsboxen $L_B = \{B_1, B_2, ... , B_n\}$ erstellt. Bei Aktualisierung des Hintergrundes wird die aktuelle Begrenzungsbox $B_{aktuell}$ mit allen Begrenzungsboxen der Liste $L_B$ verglichen. Wenn eine Begrenzungsbox $B_i$ in $L_B$ nicht mit der aktuellen Begrenzungsbox $B_{aktuell}$ überschnitt, wird das Unterbild in $B_i$ von aktuellem Bild $I$ in das Hintergrundbild $H$ in der gleichen Position ersetzt und in eine Liste $L^*_B$ verschoben. Auf den Abbildungen \ref{fig:mymethod1}h, \ref{fig:mymethod2}h, \ref{fig:mymethod3}h werden die Unterbilder mit Begrenzungsboxen in $L^*_B$, die mit rot markiert sind, in das Hintergrundbild ersetzt. Die Liste $\hat{L}_B = L_B \backslash L^*_B$ enthält alle vergangenen Begrenzungsboxen, die nicht mit der aktuellen Begrenzungsbox $B_{aktuell}$ überschneiden. Die Liste $\hat{L}_B$ wird auf den Abbildungen \ref{fig:mymethod1}g, \ref{fig:mymethod2}g, \ref{fig:mymethod3}g mit weißer Farbe dargestellt. Außerdem wird die Begrenzungsbox $B_{aktuell}$ mit grüner Farbe auf den Abbildungen \ref{fig:mymethod1}f, \ref{fig:mymethod2}f, \ref{fig:mymethod3}f gezeigt. Nach jeder Aktualisierung des Hintergrundbildes werden alle Elemente der Liste $L^*_B$ gelöscht und die Liste $L_B$ enthält nur die Begrenzungsboxen, die mit der aktuellen Begrenzungsbox $B_{alktuell}$ nicht überschneiden. \\
Hier führt zu der Frage, warum bei der Aktualisierung des Hintergrundmodells die Begrenzungsboxen statt der erstellten Silhouetten genommen. Die Antwort dafür ist, wenn die Silhouetten gespeichert werden, wird es sehr viel Speicherbedarf benötigt. Für eine beliebige Begrenzungsbox werden nur ihre Position, Breite und Höhe gespeichert und es wird für eine Silhouette jedoch alle Pixelpositionen gebraucht.    
\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\textwidth]{fig/mymethode4edited.pdf}
	\caption{Video: \glqq{}fallen.mp4\grqq{}. Frame-Nummer: 334. Die vergangene Begrenzungsbox $B_i$, die das rote Rechteck ist, überschneidet mit der aktuellen Begrenzungsbox $B_{aktuell}$, die das grüne Rechteck ist. Das Unterbild in dem roten Rechteck wird auf dem Hintergrundbild aktualisiert.}
	\label{fig:mymethod1}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\textwidth]{fig/mymethode3edited.pdf}
	\caption{Video: \glqq{}fallen.mp4\grqq{}. Frame-Nummer: 335. Nach der Aktualisierung des Hintergrundbildes wird die vergangene Begrenzungsbox $B_i$ gelöscht.}
	\label{fig:mymethod2}
\end{figure}
\begin{figure}[H]			
	\centering
	\includegraphics[width=0.9\textwidth]{fig/mymethode2edited.pdf}
	\caption{Video:\glqq{}fallen.mp4\grqq{}. Frame-Nummer: 403. Die Liste der vergangenen Begrenzungsboxen, die nicht mit dem aktuellen Begrenzungsbox $B_{aktuell}$ überschneiden, werden mit der weißen Rechtecken dargestellt. Die Liste der vergangenen Begrenzungsboxen, die mit dem aktuellen Begrenzungsbox $B_{aktuell}$  überschneiden, werden mit der roten Rechtecken dargestellt 
	}
	\label{fig:mymethod3}
\end{figure}

\subsubsection{Zweite Verbesserung (KNN-Plus-ASB)}
Die ASB Methode hat zwei Vorteile. Der erste Vorteil ist, dass die Methode eine Lösung des Problems ist, wenn eine Person beispielsweise stillsteht. Nach einer bestimmter Zeit wird die Person in das Hintergrund integriert. Der zweite Vorteil ist eine Verbesserung der Verarbeitungszeit, da bei anderer Methode aus Kapitel \ref{sec:grunglagen_hintergrundsub} eine Liste von Hintergrundbildern gebraucht wird, um ein Hintergrundmodell zu bauen. Aber mit der obengenannten Verbesserung sind nur das Hintergrundbild und das aktuelle Bild nötig.\\
Die erste Verbesserung hat auch einen Nachteil. Weil das Hintergrundmodell nur an Teile des aktuellen Bildes aktualisiert, wird kleine Änderung des Hintergrundbildes nicht betrachtet. Auf diesem Grund gibt es viel Rauschen an dem Binärbild nach der Hintergrundsubtraktion. Die zweite Verbesserung ist eine Kombination von \acs{KNN} und der ersten Verbesserung. Diese zweite Verbesserung basiert auf einer Kombination der Hintergrundsubtraktion von \acs{KNN} mit der Aktualisierungsvorschrift des Hintergrundes meiner eigenen Methode. Das heißt, wenn eine Person sich bewegt in Szene bewegt, wird die Person mit einer Box markiert. Das Teil von Hintergrund, wo die Box stattfindet, wird nicht bei Aktualisierung von Hintergrund betrachtet. Der Unterschied bei Aktualisierung des Hintergrundbildes zwischen meine eigene Methode und  die Verbesserung durch \acs{KNN} ist:    
\begin{itemize}
	\item ASB Methode: Durch eine Bewegung wird das Hintergrundbild nur an die vergangenen Begrenzungsboxen (weißen Boxen in Abbildungen \ref{fig:mymethod1} g, \ref{fig:mymethod2} g, \ref{fig:mymethod3}g), die nicht mit aktueller Bewegung überschneiden, aktualisiert.
	\item Verbesserung mit KNN-Plus-ASB: Ein Binärbild wird mit der \acs{KNN} Methode erstellt. Aus dem Binärbild wird eine Silhouette mit einer Begrenzungsbox beschränkt. Das ganze Hintergrundbild außer dem Bereich der Begrenzungsbox wird aktualisiert.
	
\end{itemize}
Diese KNN-Plus-ASB verbessert die Qualität von Silhouette und deswegen erhöht auch die Genauigkeit der Schätzung der Körperhaltung, die in nächstem Abschnitt besprochen wird.  Allerdings, die Kombination von \acs{KNN} und meine eigene Methode hat einen Nachteil, dass die Laufzeit länger als erster Versuch mit eigene Methode. Die beiden Methoden wurden auf einem Rechnern mit Intel i5 2.60 GHz Prozessor durchgeführt.
Die Laufzeit betragt vorher durchschnittlich 45 Millisekunden für ein Bild und nachher gegen 100 Millisekunden, aber es ist noch akzeptierbar für eine Echtzeitanwendung.
\newpage
\subsection{Ergebnisse der eigene Hintergrundsubtraktion}
Um die Qualität von erstellte  Silhouetten-Erkennung zu bewerten, wurden die \glqq{}Mean Squared Error\grqq{} und \glqq{}Structural Similarity Measure\grqq{} zum Einsatz gebracht. Die \glqq{}Ground Truth\grqq{}-Bilder wurden manuell erstellt und mit den Silhouetten von ASB und KNN-Plus-ASB verglichen.  Die Formel von \glqq{}Mean Squared Error\grqq{} wird wie folgt beschrieben\cite{kapadia2017mathematical}:\\

\begin{equation}
MSE = \frac{1}{m n } \sum \limits_{i=0}^{m -1} \sum \limits_{j=0}^{n -1} [ I(i, j) - K (i, j)]^2
\end{equation}
wobei $m$ und $n$ die Breite und Höhe des Bildes sind und $MSE$ als durchschnittliche Summe von quadratischem Differenz von jeden Pixel berechnet wird. Je kleiner der $MSE$ Wert ist, desto ähnlicher sind die zweit verglichen Bilder.  \glqq{}Structural Similarity Measure\grqq{} wird in \cite{wang2004image} vorgestellt. Die menschliche Wahrnehmung ist stark angepasst, um strukturelle Informationen aus einer Szene zu extrahieren, deswegen ist\glqq{}Structural Similarity Measure\grqq{} für die Qualitätsbewertung verwendet, der auf der Strukturinformationen basiert und wie folgt berechnet\cite{wang2004image}:
\begin{equation}
SSIM (x, y) = \frac{(2\mu_x \mu_y + c_1)(2\sigma_{xy} + c_2)}{(\mu_x^2 + \mu_y^2 + x_1) ({\sigma_x}^2 + {\sigma_y}^2 + c_2)}
\end{equation}
wobei $x=\{x_i | 1, 2 \dots, N\}$ und $y=\{y_i|1, 2 \dots, N\}$ die Positionen des NxN Fensters in jedem Bild beschreibt. $\mu_x$ und $\mu_y$ sind  die Mittelwerte der Pixelintensität in x und y Richtung.  $\sigma_x$ und $\sigma_y$ sind Varianzen,  $\sigma_{xy}$ ist Kovarianz von $x$ und $y$. $C_1$ und $C_2$ sind zwei Konstante.  SSIM ist zwischen 0 und 1 beschränkt und je höher der Wert von SSIM ist, desto ähnlicher sind die zwei Bilder. Tabelle \ref{tbl:comparesihoulette} zeigt uns den Unterschied zwischen dem ASB und KNN-Plus-ASB. Es ist deutlich, dass die zweite Verbesserung bessere Ergebnisse liefern kann, weil der \glqq{}MSE\grqq{} Wert kleiner und der \glqq{}SSIM\grqq{} Wert größer ist.

\begin{table}[H]
	\begin{center}
		\begin{tabular}{ c  c  c  c  }
			\toprule
			Original & Ground true & 1. Verbesserung & 2. Verbesserung \\ 
			\bottomrule
			\raisebox{-\totalheight}{\includegraphics[width=0.25\textwidth]{fig/original1.png}}
			& 
			\raisebox{-\totalheight}{\includegraphics[width=0.25\textwidth]{fig/groundtrue1.png}}
			& 
			\raisebox{-\totalheight}{\includegraphics[width=0.25\textwidth]{fig/My1.png}}
			&
			\raisebox{-\totalheight}{\includegraphics[width=0.25\textwidth]{fig/MOG1.png}}
			\\
			\\
			& & MSE:252,12 SSIM: 0,98 & MSE: 109,36 SSIM: 0,99\\
			 \bottomrule

			\raisebox{-\totalheight}{\includegraphics[width=0.25\textwidth]{fig/original2.png}}
			& 
			\raisebox{-\totalheight}{\includegraphics[width=0.25\textwidth]{fig/groundtrue2.png}}
			& 
			\raisebox{-\totalheight}{\includegraphics[width=0.25\textwidth]{fig/My2.png}}
			&
			\raisebox{-\totalheight}{\includegraphics[width=0.25\textwidth]{fig/MOG2.png}}
			\\
			\\
			& & MSE:459,49 SSIM: 0,98 & MSE: 158,21 SSIM: 0,99\\
			\bottomrule
		
			\raisebox{-\totalheight}{\includegraphics[width=0.25\textwidth]{fig/original3.png}}
			& 
			\raisebox{-\totalheight}{\includegraphics[width=0.25\textwidth]{fig/groundtrue3.png}}
			& 
			\raisebox{-\totalheight}{\includegraphics[width=0.25\textwidth]{fig/My3.png}}
			&
			\raisebox{-\totalheight}{\includegraphics[width=0.25\textwidth]{fig/MOG3.png}}
			\\
			\\
			& & MSE:268,24 SSIM: 0,99 & MSE: 121,59 SSIM: 0,99\\
			\bottomrule
			
			
			\raisebox{-\totalheight}{\includegraphics[width=0.25\textwidth]{fig/original4.png}}
			& 
			\raisebox{-\totalheight}{\includegraphics[width=0.25\textwidth]{fig/groundtrue4.png}}
			& 
			\raisebox{-\totalheight}{\includegraphics[width=0.25\textwidth]{fig/My4.png}}
			&
			\raisebox{-\totalheight}{\includegraphics[width=0.25\textwidth]{fig/MOG4.png}}
			\\
			\\
			& & MSE:479,95 SSIM: 0,97 & MSE: 111,75 SSIM: 0,99\\
			\bottomrule
			
			\raisebox{-\totalheight}{\includegraphics[width=0.25\textwidth]{fig/original5.png}}
			& 
			\raisebox{-\totalheight}{\includegraphics[width=0.25\textwidth]{fig/groundtrue5.png}}
			& 
			\raisebox{-\totalheight}{\includegraphics[width=0.25\textwidth]{fig/My5.png}}
			&
			\raisebox{-\totalheight}{\includegraphics[width=0.25\textwidth]{fig/MOG5.png}}
			\\
			\\
			& & MSE:182,61 SSIM: 0,99 & MSE: 54,42 SSIM: 1.0\\
			\bottomrule
		\end{tabular}
		\caption{Qualität der Silhouette zum Vergleichen von ASB und KNN-Plus-ASB. Je kleiner der \glqq{}MSE\grqq{} Wert desto besser ist das Ergebnis. Je größer der \glqq{}SSIM\grqq{} Wert desto besser ist das Ergebnis.}
		\label{tbl:comparesihoulette}
	\end{center}
\end{table}

\section{Schätzung der Körperhalterung mittels Histogrammanalyse}\label{chp:schatzung}
Im Abschnitt \ref{chp:BackgroundSubtraction} wurde eine Silhouette von einer Person durch Hintergrundsubtraktion erstellt und in diesem Abschnitt wird der zweite Meilenstein von meiner Arbeit beschrieben. Es geht um Schätzung der Körperhaltung mit Hilfe von Histogrammanalyse. Wie schon im Kapitel \ref{chp:Grundlageb} beschrieben, basiert die Schätzung der Körperhaltung in meiner Arbeit auf die Forschung von Haritaoglu, Hatwood und Davis in \cite{haritaoglu1998ghost}. Die Histogrammanalyse zur Schätzung der Körperhaltung in dem sogenannten \glqq{}Ghost\grqq{}-System in \cite{haritaoglu1998ghost} wird hier angewendet. Nach der Hintergrundsubtraktion wird ein Binärbild kreiert, das Bild stellt  eine Bewegung in der Szene dar. Die Bewegung wird durch eine Begrenzungsbox eingeschränkt (Abbildungen \ref{fig:mymethod1}f, \ref{fig:mymethod2}f, \ref{fig:mymethod3}f). Mit Hilfe von dieser Begrenzungsboxen werden die Silhouette von dem ganzem Bild extrahieren und wird ein Unterbild erschaffenen, das nur die Silhouette enthält. 
Um bei der Histogrammanalyse vergleichbare Ergebnisse für unterschiedlich große Begrenzungsboxen zu bekommen, wird eine Skalierung angewendet. Nach der Erzeugung von dem Subimage wird das Ergebnis zuerst auf 128 Pixel skaliert. Falls die Breite größer als Höhe des Bildes ist, wird die Breite auf 128 Pixel skaliert und die Höhe wird auch skaliert, sodass das Verhältnis zwischen Breite und Höhe nicht geändert wird. Wenn die Höhe größer als die Breite des Bildes ist, wird die Höhe analog skaliert.
\\


Nach der Skalierung werden die Histogramme von horizontale sowie vertikale Achse gerechnet. Wenn die Höhe größer als die Breite des geschnitten Unterbild ist, wird das Unterbild auf 128 Pixel Höhe skaliert. Die Breite des skalierten Unterbildes in  diesem Fall ist kleiner als 128 Pixel. Auf diesem Grund ist das vertikale Histogramm des Unterbilder auch kleiner als 128 Einheiten bei x-Achse. Dies vertikale Histogramm wird in de




Bei eine kürzere Achse von $x$ und $y$ Achsen wird die Histogramm in der Mitte von 128 Pixel lokalisiert und die überflüssige Positionen an der Histogramm werden mit Null erfüllt. Wie schon im Abschnitt \ref{sec:Histogrammanalyse} beschrieben wird, werden mit der Formel \ref{eq:loglikelyhood} die berechneten Histogramme mit vordefiniert normalisierte Referenz-Histogramme (siehe Abbildung \ref{fig:histogramm}) verglichen.  Je kleiner das Ergebnis aus Formel \ref{eq:loglikelyhood} ist, desto höher ist die Wahrscheinlichkeit, dass die Person die Körperhaltung hat.
Das Ergebnis von Körperhaltung basiert auf die Ähnlichkeit zwischen den Histogramme mit Hilfe Loglikelihood-Funktion. Um den zweiten Hauptschritt, stellt die Abbildung \ref{fig:schatzung} konkret wie die Schätzung der Körperhaltung dar. Die Abbildungen \ref{fig:schatzung1}, \ref{fig:schatzung2} und \ref{fig:schatzung3} repräsentieren die Schätzung der Körperhaltung in graphische Beispiele, um den zweiten Schritt klarzustellen.
\begin{figure}[H]
	\centering	
	\includegraphics[width=0.95\textwidth]{fig/schatzung.pdf}
	\caption{Der zweiten Meilenstein ist die Schätzung der Körperhaltung. Das Ausgabebinärbild des ersten Meilensteins wird mit einer Begrenzungsbox beschränkt, um ein Unterbild von der Silhouette zu erstellen. Das Unterbild wird dann skaliert und die vertikale und horizontale Histogramme des skalierten Unterbildes werden berechnet. Diese Histogramme werden mit den Referenz-Histogrammen, um die Körperhaltung auf dem Unterbild zu bestimmen.}
	\label{fig:schatzung}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{fig/schatzung1.pdf}
	\caption{Beispiel für Prozess von Schätzung einer Silhouette bei Stehen. Die Silhouette des Binärbildes wird zuerst durch eine Begrenzungsbox extrahiert. Die vertikale und horizontale Histogramme der Silhouette werden dann berechnet und mit den entsprechenden Referenz.Histogrammen jeder generischen Körperhaltung verglichen. Die Körperhaltung, die nach dem Vergleichen einen größten Wert hat, ist für die Silhouette genommen.}
	\label{fig:schatzung1}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{fig/schatzung2.pdf}
	\caption{Beispiel für Prozess von Schätzung einer Silhouette bei Liegen}
	\label{fig:schatzung2}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{fig/schatzung3.pdf}
	\caption{Beispiel für Prozess von Schätzung einer Silhouette bei Beugen}
	\label{fig:schatzung3}
\end{figure}


Um die Zuverlässigkeit von diesem Schritt zu überprüfen, wird die Körperhaltung im Testvideos Bild für Bild annotiert, auf welchem Bild und auf welcher Position in der Szene die Testperson liegt, steht, sitzt und beugt. Das ergibt eine Genauigkeit von mehr als $70\%$. Die graphische Darstellung in Abbildungen \ref{fig:schatzeva} und \ref{fig:schatzeva2} zeigt das Ergebnis der Schätzung der Körperhaltung. Die Körperhaltung wird als Nummern zugeordnet, Null seht für Stehend, Eins steht für Liegend, Zwei steht für Beugend und Drei steht für Sitzend. Wenn die Schätzung der Körperhaltung minus Eins sich ergibt, dann bedeutet, dass keine Person mehr in der Szene ist. Die blaue Gerade ist der Referenz-Wert in Testvideos und die gelbe Gerade ist das Ergebnis von unserem Programm. Anhand der graphische Darstellungen (siehe Abbildung \ref{fig:schatzeva} und \ref{fig:schatzeva2}) kann es festgestellt werden, dass in manche Stelle die Schätzung mit einer anderen falschen Körperhaltung und dann wieder auf die richtige Körperhaltung bewertet wurde. Das Problem beeinflusst nicht an die Endergebnisse, weil die außergewöhnliche Situationen in einem Zeitraum statt einem Zeitpunkt betrachtet werden. Beispielsweise werden die Körperhaltung in Abbildung \ref{fig:schatzeva2} von Bild 700 bis 850 betrachtet. Es gibt zwei Stellen, an den die Körperhaltung von dem Zustand \glqq{}Stehen\grqq{} auf \glqq{}Sitzen\grqq{} oder \glqq{}Beugen\grqq{} falsch bewertet. Weil der Fokus von meiner Arbeit nicht speziell an Schätzung der Körperhaltung sondern an Erkennung der außergewöhnlichen Situationen ist, wird das Problem mit den minimalen falschen Erkennungen im Graph akzeptierbar.
			
\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{fig/schatzungevaluation.png}
	\caption{Überprüfung der Schätzung der Körperhaltung an Testvideo \glqq{}fallen2.mp4\grqq{}. Die x-Achse stellt die Framenummer dar und die y-Achse ziegt die Indexen der Körperhaltung. Die blaue Gerade ist der Referenz-Körperhaltung und die gelbe Gerade ist die geschätzte Körperhaltung. Die Schätzung der Körperhaltung hat in diesem Beispiel ein Genauigkeit von $76,47\%$} 
	\label{fig:schatzeva}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{fig/schatzungevaluation2.png}
	\caption{Überprüfung der Schätzung der Körperhaltung an \glqq{}fallen2.mp4\grqq{}. Die Schätzung der Körperhaltung hat in diesem Beispiel ein Genauigkeit von $71,93\%$} 
	\label{fig:schatzeva2}
\end{figure}

\section{Erkennung außergewöhnlicher Situation}
Im Abschnitt \ref{chp:BackgroundSubtraction} und \ref{chp:schatzung} wird eine Silhouette durch Bewegungen erstellt und eine Schätzung der Körperhaltung durch die Histogrammanalyse berechnet. Zur Erkennung außergewöhnlicher Situationen wird noch die aktuelle Position der Person in einem bestimmten Zeitraum berücksichtigt werden. An dieser Stelle ist es benötigt, dass die Anwendung den Unterschied zwischen normalen Bewegungen im Alltag und außergewöhnlichen Situationen erkennen soll. Wenn beispielsweise die Körperhaltung einer Person als \glqq{}Liegend\grqq{} geschätzt ist, ist es möglich, dass die Person auf einem Sofa oder in einem Bett schläft. Wenn eine Person im Bett liegt, handelt es sich um eine normale Situation. Deswegen ist es wichtig zur Identifizierung der Positionen der Möbeln im Raum. Zur Erkennung außergewöhnlicher Situationen gab es ein paar Ideen. Zum Beispiel: Der Raum, indem die Kamera gestellt ist, ein 3D-Modell restrukturiert werden kann. Diese Idee hat einen Vorteil, dass die Positionen von Möbeln im Raum identifiziert werden können, um zu bestimmen, ob die Person im Bett oder auf der Boden liegt. Die Arbeit für Restrukturierung ist sehr zeitaufwendig und es ist nötig, für jeden neuen Raum ein 3D-Modell aufzubauen. Es gibt noch eine andere Technik zur Identifizierung der Positionen der Möbeln im Raum und die Methode ist als \glqq{}Bird Eye View\grqq{} genannt. Die Technik ist bei Automobilen mit Rückfahrkamera angewendet. Der Nachteil von der Technik ist: Der Abstand zwischen der Kamera und dem Boden muss festgestellt werden. Auf diesem Grund ist diese Technik für dies Projekt nicht anwendbar, weil die Kamera an einer beliebigen Position im Raum gestellt werden soll. Die beste Lösung ist die einfachste, die funktioniert. Um das genannte Problem bei Unterscheidung von normalen und abnormalen Situationen zu lösen, wird in diesem Meilenstein eine logische Methode, die als Fuzzylogik genannt ist, verwendet. Die Positionen der Möbeln im Raum sind auf dem Bild vordefiniert. Die Fuzzylogik berechnet durch die Positionen der Möbeln (zum Beispiel: Bett oder Sofa), aktuelle Körperhaltung und aktuelle Zeit dann die Wahrscheinlichkeit, dass eine außergewöhnliche Situation passiert. Wie in Abschnitt \ref{sec:fuzzylogik} schon erwähnt, geht Fuzzylogik um eine unscharfe logische Menge. Fuzzylogik besteht aus drei Schritte: Umwandlung von Eingaben in Fuzzymenge, Anwendung von vordefinierten (IF-ELSE) Regeln und Defuzzifizierung (siehe Abbildung \ref{fig:fuzzylogik}).

\begin{figure}[H]
	\centering
	\includegraphics[width=0.85\textwidth]{fig/fuzzylogik.pdf}
	\caption{Dritter Meilenstein}
	\label{fig:fuzzylogik}
\end{figure}

Zuerst müssen die Mitgliederfunktionen der Eingaben definiert werden und die sind wie folgt beschrieben:

\begin{itemize}
	\item Die Körperhaltung besteht aus vier Zustände: Stehend (Index: 0), Liegend (Index: 1), Beugend (Index: 2), Sitzend (Index: 3) und die sind als Punkte im Fuzzymodel definiert.
	\item Die Zeit besteht aus Tag und Nacht und ist mit glockenförmige Funktionen mit Parameter $a=6$, $b=10$ und $mean=14$ für Tag und $mean=2$ für Nacht definiert. Der Wertbereich für die Zeit ist ein Intervall [0, 24] für 24 Stunden am Tag. Die glockenförmige Funktion kann wie folgt beschrieben\cite{rosenfeld1971fuzzy}:
	\begin{equation}
		f(a, b, mean) = \frac{1}{1 + |\frac{x - mean}{a}|^{2b}}
	\end{equation} 
	\item Die normale Koordinaten im Raum sind die Positionen, an den die Testperson normalerweise liegt, sitzt oder steht und die wird mit Gaußschen-Funktionen definiert, wobei die Mittelwerte der Funktion $xy$-Koordinaten sind und die Standardabweichung  $30$ genommen ist.
	\item Die Ausgabe ist als \glqq{}Status\grqq{} genannt. \glqq{}Status\grqq{} hat eine Gaußschen-Funktion mit Standardabweichung $\sigma = 1$ Mittelwert $\mu=4$ für abnormale und $\mu=6$ für normale Situation.
\end{itemize}  

Die allen oben vordefinierten Methoden werden auf die Abbildung \ref{fig:fuzzyfunktion} graphisch dargestellt. Zur Anwendung von (IF-ELSE) Regeln (siehe Algorithmus \ref{algo:fuzzy_modell} ) sind Minimum als Aktivaktionsmethode und Maximum als Akkumulationsmethode eingesetzt. Die Regeln in Algorithmus \ref{algo:fuzzy_modell} kann wie folgt interpretiert werden:

\begin{itemize}
	\item Es ist schlecht, wenn die Person nicht in Sofa liegt.
	\item Es ist gut, wenn die Person in Sofa liegt.
	\item Es ist schlecht, wenn die Person am Tag liegt.
	\item Es ist gut, wenn die Person am Tag steht.
	 \item Es schlecht, wenn die Person am Tag sitzt oder beugt.
\end{itemize}

Bei Defuzzifizierung, die eine Abbildung der Ausgabe in scharfe Menge durchführt, wird die Schwerpunktmethode verwendet. Die oben vordefinierten Parameter können anhand des Raum, biologischer Uhr der Testperson angepasst werden.\\

Die Fuzzylogik berechnet, wie hoch die Wahrscheinlichkeit ist, dass die aktuelle Situation um einen normalen Fall geht. Für das Beispiel im Abbildung \ref{fig:Fuzzy_Example} sind Dreieckfunktionen angewendet, um die Funktion der Fuzzylogik einfacher zu verstehen. Für die komplexeren Anwendungsfälle sind Gaußschen-Funktionen benutzt, um ein Demo zu realisieren. Es ist angenommen, dass die Koordinaten von einem Sofa in Testraum $(591, 441)$, die aktuelle Zeit $0$ Uhr Nacht und $8$ Uhr Morgen sind und die Testperson liegt im Position (574, 424) (siehe Abbildung \ref{fig:fuzzyfunktion}). In dem ersten Testfall, indem die Testperson an dem Sofa in der Nacht liegt, ergibt sich die Unterscheidung für normale Situation ein Ergebnis von $93,29\%$ und für eine außergewöhnliche Situation ein Ergebnis von $26,59\%$ (Abbildung \ref{fig:fuzzy1}). Es ist klar, dass dies Fall eine normale Situation ist, weil anhand den Regeln in Algorithmus \ref{algo:fuzzy_modell} die Ausgabe der Fuzzylogik \glqq{}good\grqq{} auskommen soll. In Regeln \ref{algo:fuzzy_modell} gibt es einen gemischten Fall, wobei die Testperson an dem Sofa um $8$ Uhr Morgen liegt. Dieser Testfall kombiniert zwei Regeln, wobei es gut ist, wenn die Testperson in normale Position liegt und es ist schlecht wenn die Person am Tag liegt. Nach der Rechnen liefert das Ergebnis einen Wert von $71,6\%$ für normale Situation und $49,71\%$ für abnormale Situation (Abbildung \ref{fig:fuzzy2}). Eine Situation wird genau dann als schlechten Situation betrachtet, wenn der Wert \glqq{}bad\grqq{} $\geq 0,8$ und $\geq$ den Wert \glqq{}good\grqq{} ist, sonst handelt es sich um eine normale Situation.

\begin{algorithm}[H]
	\caption{Regel für Raumtemperatur und Einstellung des Thermostates.	}
	\label{algo:fuzzy_modell}
	\If{(posture IS laying) AND (NOT xposition IS good) OR (NOT yposition IS good)}{status IS bad;}
	\If{(posture IS laying) AND (xposition IS good) AND (yposition IS good)}{status IS good;}
	\If{posture IS laying AND time IS day}{status IS bad;}
	\If{posture IS standing AND time IS day}{status IS good;}
	\If{(IF posture IS sitting OR posture IS bending) AND time IS night}{status IS bad;}
\end{algorithm}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{fig/fuzzyfunktion.png}
	\caption{Die Mitgliederfunktionen der Körperhaltung, Zeit und aktuellen Position der Person. Der Eingabebereich der Zeit ist von 0 bis 24 beschränkt und die Eingabe der Position muss ein positiver Wert sein.} 
	\label{fig:fuzzyfunktion}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.57\textwidth]{fig/fuzzy1.png}
	\caption{Testfall: Person liegt im Sofa um 12 Uhr Nacht. Das Programm liefert ein Ergebnis von 93,29\% für normale und 26,59\% für abnormale Situation.} 
	\label{fig:fuzzy1}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.57\textwidth]{fig/fuzzy2.png}
	\caption{Testfall: Person liegt im Sofa um 8 Uhr morgen. Das Programm liefert ein Ergebnis von 71,6\% für normale und 49,71\% für abnormale Situation.} 
	\label{fig:fuzzy2}
\end{figure}

Bis jetzt können die außergewöhnlichen Situationen von normale Bewegungen im Alltag mit Hilfe von Fuzzylogik unterschieden werden. Die außergewöhnliche Situationen sollen nicht nur in einem Bild sondern in einer Sequenz von mehreren Bildern beachtet, damit das System bessere Entscheidung für eine abnormale Situation treffen kann. Wenn nur ein Bild beachtet würde, würde das System sofort nach einem Bild falsch alarmiert. Um die Beobachtung in mehreren Bildern zu realisieren, wird ein Modell gebaut, wobei die Person in einer Begrenzungsbox in einem Zeitraum betrachtet wird.  Das heißt, jede \inlinecode{C++}{Person}-Objekt besteht aus einer aktuellen Begrenzungsbox $B_{aktuell}$, die nach der Hintergrundsubtraktion erstellt ist. Nach jedem eingegangenen Bild wird die neue Begrenzungsbox $B_{neu}$ mit der aktuellen Begrenzungsbox $B_{aktuell}$ verglichen, um zu bestimmen, ob es sich um die selbe Person handelt und ob die Person sich bewegt. Eine globale Variable \inlinecode{C++}{movementMaximum} wird verwendet, um zu garantieren, dass  die neue Begrenzungsbox zu der gleichen Person von dem letzten Bild gehört. Der Abstand zwischen der aktuellen Begrenzungsbox $B_{aktuell}$ und  der neuen Begrenzungsbox $B_{neu}$ wird berechnet und mit der \inlinecode{C++}{movementMaximum} verglichen. Wenn dieser Abstand größer als \inlinecode{C++}{movementMaximum} ist, wird eine neue Person erstellt und wird die aktuelle Person ignoriert. Wenn dieser Abstand kleiner oder gleich  \inlinecode{C++}{movementMaximum} ist, wird die aktuelle Begrenzungsbox $B_{aktuell}$ der Person mit der neuen Begrenzungsbox $B_{neu}$ ersetzt (siehe Algorithmus \ref{algo:gleicheperson}).

\begin{algorithm}[H]
\caption{Bedingung zur Erkennung von gleicher Person}
\label{algo:gleicheperson}	
\begin{algorithmic}
\IF { $|$ Koordinaten von $B_{aktuell}$ - Koordinaten von $B_{neu} | \leq$  \inlinecode{C++}{movementMaximum}}
	\STATE $B_{aktuell}$ der Person $\gets$ $B_{neu}$
\ELSE 
	\STATE Erstelle neue Person mit Begrenzungsbox $B_{neu}$
\ENDIF
\end{algorithmic}
\end{algorithm}

Die nächste globale Variable \inlinecode{C++}{movementMinimum} gewährleistet, ob die Person sich bewegt. Diese Variable hilft uns bei der Erkennung von außergewöhnlicher Situation, in der die Person liegt und sich nicht mehr bewegt. Wenn der Abstand zwischen der aktuellen Begrenzungsbox $B_{aktuell}$ und  der neuen Begrenzungsbox $B_{neu}$ kleiner als \inlinecode{C++}{movementMinimum}, bedeutet es, dass die Person sich nicht mehr bewegt und umgekehrt. Außerdem sind die Breite und Höhe von $B_{aktuell}$ und $B_{aktuell}$ auch verglichen, um zu bestimmen, dass es eine Änderung der Begrenzungsbox gibt, wenn sie trotzdem an einer gleichen Position bleibt. Um zu berechnen, ob die Person sich in einer gleichen Position nicht bewegt, wird \inlinecode{C++}{monvementMinimum} in dem Algorithmus benutzt:\\

\begin{algorithm}[H]
	\caption{Bedingung zur Erkennung von Bewegung einer Person}
	\label{algo:gleicheposition}	
	\begin{algorithmic}
		\IF {$|$Koordinaten neuer Box  - Koordinaten  alter Box$|$ $\geq$ \inlinecode{C++}{monvementMinimum} $\OR$ 
		$|$Breite neuer Box  - Breite alter Box$|$ $\geq$ \inlinecode{C++}{monvementMinimum} $\OR$
		$|$Höhe neuer Box  - Höhe alter Box$|$ $\geq$ \inlinecode{C++}{monvementMinimum}
	}
		\STATE Person bewegt sich
		\ELSE 
		\STATE Person bewegt sich NICHT
		\ENDIF
	\end{algorithmic}
\end{algorithm}

Mit dem beschriebenen \inlinecode{C++}{Person}-Modell kann eine Person in einem Zeitraum betrachtet werden und dann kann das System die Notfälle besser bewerten. Bis jetzt kann das System eine außergewöhnliche Situation mit folgenden Bedingungen erkennen:\\
\begin{itemize}
	\item Durch Hintergrundsubtraktion, Schätzung der Körperhaltung wird eine Person erkannt, ob er steht, liegt, beugt oder sitzt.
	\item Durch Fuzzylogik kann das System bewerten, ob eine Person in einem normalen oder nicht normalen Ort liegt.
	\item Durch das \inlinecode{C++}{Person}-Modell kann das System eine Person folgen und erkennen, dass eine Person in einem bestimmten Zeitraum sich bewegt oder nicht.
\end{itemize}

Nun kann eine kurze Zusammenfassung der oben gemeinten Bedingungen erläutert werden, damit ein komplettes System zur Erkennung einer außergewöhnlichen Situation erzeugt werden kann. Es handelt sich um einen Notfall genau dann, wenn eine Person in einem abnormalen Ort liegt und sich nicht mehr in einem Zeitraum nicht mehr bewegt. Der Algorithmus \ref{algo:finalAlgo} mach den Weg zur Erkennung der außergewöhnlicher Situation deutlich. Nach dem Aufnahme eines neuen Bild wird eine Silhouette durch Hintergrundsubtraktion erstellt. Die erstellte Silhouette ist die Eingabe der Histogrammanalyse und die Ausgabe der Histogrammanalyse ist eine geschätzte Körperhaltung. Danach wird eine Fuzzylogik mit der Eingaben: die Begrenzungsbox der Silhouette, die geschätzten Körperhaltung und die aktuellen Zeit erstellt. Die Ausgabe der Fuzzylogik ist die Wahrscheinlichkeit, dass eine Person in einer normale oder abnormale Situation ist. Durch die Begrenzungsbox der Silhouette wird es berechnet, ob es sich um eine gleiche Person wie auf letzten Bild handelt (siehe Algorithmus \ref{algo:gleicheperson}). Wenn die gleiche Person detektiert, wird die aktuellere Begrenzungsbox der Person $B_{aktuell}$ mit der neuen Begrenzungsbox $B_{neu}$ ersetzt. Außerdem wird die Person noch betrachtet, ob er liegt in einem abnormale Ort liegt und sich nicht bewegt (siehe Algorithmus \ref{algo:gleicheposition}). Wenn ein außergewöhnlicher Fall auf einem Bild erkannt, wird einen Zähler (\glqq{}badcounter\grqq{} genannt) sich erhöhen, sonst wird der Zähler zurückgesetzt. Am Ende wird der Zähler mit einer bestimmter Zeit-Variable verglichen. Wenn der Zähler größer als die vordefinierte Zeit ist, wird ein Alarm ausgelöst. 

\begin{algorithm}[H]
\caption{Zusammengefasster Algorithmus}
\label{algo:finalAlgo}	
\KwData{b : Aktuelles Bild}
\KwResult{Erkennung außergewöhnlicher Situation}
\begin{algorithmic}
\STATE $s:silhouette$, $B_{neu}:begrenzungsbox$ $\gets$ Hintergensubtraktion($b$)
\STATE $k:koerperhaltung$ $\gets$ Histogrammanalyse($s$)
\STATE $t:zeit$ $\gets$ aktuelle Zeit
\STATE $z:zustand$ $\gets$ Fuzzylogik($B_{neu}$, $ k$, $t$)
\\
\COMMENT{Zustand hat zwei Werte \glqq{}gut\grqq{} und \glqq{}schlecht\grqq{}, es sagt wie gut die Situation nach Fuzzymodell bewertet wird}

\IF {Gleiche Person detektiert wird}
\STATE $person$ $\gets$ die detektierte Person
\STATE Aktualisierung der Begrenzungsbox $B_{aktuell}$ der $person$

	\IF{$person$ bewegt sich nicht $\AND$ $k$ = LIEGEN $\AND$ $z$ = SCHLECHT}
	\STATE $person.badcounter$ ++
	\ELSE
	\STATE $person.badcounter$ $\gets$ 0
	\ENDIF
\ELSE
\STATE Erstelle neue Person
\STATE $person$ $\gets$ neue erstellte Person
\STATE $B_{aktuell}$ der $person$ $\gets$  $B_{neu}$
\ENDIF

\IF{$person.badcounter$ $\geq$ bestimmte Zeit}
\STATE Löse einen Alarm aus.
\ENDIF

\end{algorithmic}
\end{algorithm}	

\section{Drehbare 360-Grad-Kamera}
Eine Eigenschaft der Bosch Innenkamera ist die Fähigkeit, rund um 360° zu drehen. Mit Hilfe von Bewegungsmeldern kann die Kamera nach eine Bewegung im Raum immer folgen.  Es bringt einen großen Vorteil für diese Arbeit, da können die außergewöhnlichen Situationen im ganzen Raum betrachtet werden. Zum Vergleich mit anderen normalen Kameras kann diese Kamera in die Richtung jeder Bewegung hinblicken. Die Kamera muss nicht in einem Ecke gestellt werden, damit sie den Raum vollständig aufnehmen kann. Die Kamera kann auf eine beliebige Stelle hingestellt werden, zum Beispiel auf dem Tisch, oder Fensterbank, darauf die Kamera mit Drehung alle Fälle beobachten kann. Wenn Kamera sich wegen einer Bewegung dreht, wird der Hintergrund komplett oder fast alles geändert und somit wird die Hintergrundsubtraktion nicht mehr funktionieren.\\
Um das Problem mit der Hintergrundsubtraktion bei Drehung der Kamera zu lösen, wird zuerst eine Hintergrunddichte mit Hilfe von OpenCV Framework berechnet. Die Hintergrunddichte repräsentiert eine Proportion des Hintergrundes auf dem Bild. Ein Schwellwert wird danach verwendet, um eine stärke Änderung des Hintergrundes bei der Drehung der Kamera zu detektieren. Angenommen ist die Hintergrunddichte von einem Bild \inlinecode{C++}{backgroundDensity}. Nach der Hintergrundsubtraktion wird ein Binärbild erstellt, auf dem die verbundenen weißen Pixel in eine Region zusammengefasst werden. Die erstellte Regionen werden sich verbreiten, solang kein weißes Pixel mehr vorhanden ist. Die acht Nachbarpixel jedes Pixels werden als verbundenen Elemente betrachtet. Das heißt, dass die Nachbarn eines Pixels in einer Region auch zur Region gehören, wenn die weiß sind. Die zusammengefassten Regionen werden von $1$ bis $n$ nummeriert (Der Hintergrund hat einen Index von $0$).\\
Mit der Funktion \inlinecode{C++}{connectedComponentsWithStats()} in OpenCV werden die verbundenen Regionen zusammengefasst und deren Fläche genau pixelweise berechnet. Wenn die Hintergrunddichte \inlinecode{C++}{backgroundDensity} kleiner als einen Schwellenwert $0.8$ ist, geht es um eine stärke Änderung des Hintergrundes. Die selbst aufgenommenen Testvideos sind mit dem Programm getestet und betragt die Dichte des Hintergrundes durchschnittlich immer über $0,8$, deshalb ist der Schwellwert $0,8$ für diesen Anwendungsfall genommen. Wenn die Bedingung  \inlinecode{C++}{backgroundDensity} $\leq 0,8$ erfüllt wird, wird das Hintergrundbild mit aktuellem Bild ersetzt. Das bedeutet, die weitere Meilensteine werden auch erneut berechnet. Nach dem Testen mit selbst aufgenommenen Videos wird ein weiteres Problem bei Drehung der Kamera entdeckt.\\
Wenn nur die Bedingung \inlinecode{C++}{backgroundDensity} $\leq 0,8$ betrachtet wird, wird ein der letzten Bildern vor dem Stillstand der Kamera als Hintergrund sofort genommen. Ein falsches Hintergrundmodell wird erstellt, trotzdem ist die genante Bedingung für die Variable \inlinecode{C++}{backgroundDensity} erfüllt. Das Programm liefert kein gutes Ergebnis, weil die Hintergrundsubtraktion ein falsches Hintergrundmodell hat. Um das Problem zu vermeiden, wird ein Auslöser \inlinecode{C++}{trigger} benutzt.  Wenn die Bedingung \inlinecode{C++}{backgroundDensity} $\leq 0,8$ erfüllt, wird der Auslöser angeschaltet und ein Zähler wird aktiviert. Nach einer Sekunde oder $30$ Bilder wird der Hintergrund erst ersetzt. Die Methode vermeidet das Problem und liefert gute Ergebnisse bei Drehung der Kamera wie bei der statischen Kamera erwartet. In die Abbildungen \ref{fig:false1} und \ref{fig:false1} sind zwei Screenshots zum Vergleichen die zwei Aktualisierungsmethode von Hintergrund. Die linke Seite der Abbildungen sind die falsche Erkennungen, wenn die Hintergrundersetzung sofort nach der Erfüllung \inlinecode{C++}{backgroundDensity} $\leq 0,8$ aktiviert wird. Wenn die Bedingung erfüllt, dreht sich die Kamera noch einen kurzen Stuck vor dem Standstil. Die rechte Seite der Abbildungen sind die Verbesserung mit einem Auslöser \inlinecode{C++}{trigger} nach $30$ Bilder. Der Algorithmus \ref{algo:newBG} macht die Hintergrundersetzung für die 360° Kamera deutlich.


\begin{algorithm}[H]
	\caption{Ersetzung des Hintergrundbildes bei Drehung der Kamera}
	\label{algo:newBG}	
	\KwData{$binaerbild \gets$ Binärbild aus Subtraktionsverfahrens}
	\KwResult{Ersetzung des Hintergrundbildes }
	\begin{algorithmic}
		\STATE $region[~] \gets$ \inlinecode{C++}{connectedComponentsWithStats}$(binaerbild)$ 
		\STATE  \inlinecode{C++}{backgroundDensity} $\gets region[0] / Bildgroeße$
		\STATE  \inlinecode{C++}{trigger} $\gets$ FALSE
		
		\IF  {\inlinecode{C++}{backgroundDensity} $ \leq 0.8$}
		\STATE \inlinecode{C++}{trigger} $\gets$ TRUE
		\STATE $counter \gets 0$
		\ENDIF
		
		\IF  {\inlinecode{C++}{trigger}}
		\STATE $counter$++
			\IF  {$counter \geq 30$ }
			\STATE Ersetzung des Hintergrundbildes mit dem aktuellen Bild. 
			\STATE Erneure \inlinecode{C++}{Person}-Modell
			\ENDIF
		\ENDIF
		
	\end{algorithmic}
\end{algorithm}	


\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{fig/false1.png}
	\caption{Linke Seite: eine falsche Erkennung der Bewegung passiert, wenn das Hintergrundbild vor dem Stillstand der Kamera ersetzt wird. Rechte Seite: eine richtige Erkennung nach der Drehung der Kamera.} 
	\label{fig:false1}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{fig/false2.png}
	\caption{Linke Seite: eine weitere falsche Erkennung der Bewegung. Rechte Seite: ein richtige Erkennung der Bewegung.} 
	\label{fig:false2}
\end{figure}
